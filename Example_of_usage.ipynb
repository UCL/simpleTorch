{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Example_of_usage.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMeJaBkXJdsp8Ps3N7xHHjc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/panos108/Training_ANN_with_Pytorch/blob/master/Example_of_usage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzL77vGM6vTg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "2dfee3f7-f4f0-43cd-a7c1-26df84a20eaf"
      },
      "source": [
        "%rm -rf Training_ANN_with_Pytorch\n",
        "!git clone https://github.com/panos108/Training_ANN_with_Pytorch.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Training_ANN_with_Pytorch'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/44)\u001b[K\rremote: Counting objects:   4% (2/44)\u001b[K\rremote: Counting objects:   6% (3/44)\u001b[K\rremote: Counting objects:   9% (4/44)\u001b[K\rremote: Counting objects:  11% (5/44)\u001b[K\rremote: Counting objects:  13% (6/44)\u001b[K\rremote: Counting objects:  15% (7/44)\u001b[K\rremote: Counting objects:  18% (8/44)\u001b[K\rremote: Counting objects:  20% (9/44)\u001b[K\rremote: Counting objects:  22% (10/44)\u001b[K\rremote: Counting objects:  25% (11/44)\u001b[K\rremote: Counting objects:  27% (12/44)\u001b[K\rremote: Counting objects:  29% (13/44)\u001b[K\rremote: Counting objects:  31% (14/44)\u001b[K\rremote: Counting objects:  34% (15/44)\u001b[K\rremote: Counting objects:  36% (16/44)\u001b[K\rremote: Counting objects:  38% (17/44)\u001b[K\rremote: Counting objects:  40% (18/44)\u001b[K\rremote: Counting objects:  43% (19/44)\u001b[K\rremote: Counting objects:  45% (20/44)\u001b[K\rremote: Counting objects:  47% (21/44)\u001b[K\rremote: Counting objects:  50% (22/44)\u001b[K\rremote: Counting objects:  52% (23/44)\u001b[K\rremote: Counting objects:  54% (24/44)\u001b[K\rremote: Counting objects:  56% (25/44)\u001b[K\rremote: Counting objects:  59% (26/44)\u001b[K\rremote: Counting objects:  61% (27/44)\u001b[K\rremote: Counting objects:  63% (28/44)\u001b[K\rremote: Counting objects:  65% (29/44)\u001b[K\rremote: Counting objects:  68% (30/44)\u001b[K\rremote: Counting objects:  70% (31/44)\u001b[K\rremote: Counting objects:  72% (32/44)\u001b[K\rremote: Counting objects:  75% (33/44)\u001b[K\rremote: Counting objects:  77% (34/44)\u001b[K\rremote: Counting objects:  79% (35/44)\u001b[K\rremote: Counting objects:  81% (36/44)\u001b[K\rremote: Counting objects:  84% (37/44)\u001b[K\rremote: Counting objects:  86% (38/44)\u001b[K\rremote: Counting objects:  88% (39/44)\u001b[K\rremote: Counting objects:  90% (40/44)\u001b[K\rremote: Counting objects:  93% (41/44)\u001b[K\rremote: Counting objects:  95% (42/44)\u001b[K\rremote: Counting objects:  97% (43/44)\u001b[K\rremote: Counting objects: 100% (44/44)\u001b[K\rremote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects:   3% (1/30)\u001b[K\rremote: Compressing objects:   6% (2/30)\u001b[K\rremote: Compressing objects:  10% (3/30)\u001b[K\rremote: Compressing objects:  13% (4/30)\u001b[K\rremote: Compressing objects:  16% (5/30)\u001b[K\rremote: Compressing objects:  20% (6/30)\u001b[K\rremote: Compressing objects:  23% (7/30)\u001b[K\rremote: Compressing objects:  26% (8/30)\u001b[K\rremote: Compressing objects:  30% (9/30)\u001b[K\rremote: Compressing objects:  33% (10/30)\u001b[K\rremote: Compressing objects:  36% (11/30)\u001b[K\rremote: Compressing objects:  40% (12/30)\u001b[K\rremote: Compressing objects:  43% (13/30)\u001b[K\rremote: Compressing objects:  46% (14/30)\u001b[K\rremote: Compressing objects:  50% (15/30)\u001b[K\rremote: Compressing objects:  53% (16/30)\u001b[K\rremote: Compressing objects:  56% (17/30)\u001b[K\rremote: Compressing objects:  60% (18/30)\u001b[K\rremote: Compressing objects:  63% (19/30)\u001b[K\rremote: Compressing objects:  66% (20/30)\u001b[K\rremote: Compressing objects:  70% (21/30)\u001b[K\rremote: Compressing objects:  73% (22/30)\u001b[K\rremote: Compressing objects:  76% (23/30)\u001b[K\rremote: Compressing objects:  80% (24/30)\u001b[K\rremote: Compressing objects:  83% (25/30)\u001b[K\rremote: Compressing objects:  86% (26/30)\u001b[K\rremote: Compressing objects:  90% (27/30)\u001b[K\rremote: Compressing objects:  93% (28/30)\u001b[K\rremote: Compressing objects:  96% (29/30)\u001b[K\rremote: Compressing objects: 100% (30/30)\u001b[K\rremote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 44 (delta 21), reused 35 (delta 14), pack-reused 0\u001b[K\n",
            "Unpacking objects:   2% (1/44)   \rUnpacking objects:   4% (2/44)   \rUnpacking objects:   6% (3/44)   \rUnpacking objects:   9% (4/44)   \rUnpacking objects:  11% (5/44)   \rUnpacking objects:  13% (6/44)   \rUnpacking objects:  15% (7/44)   \rUnpacking objects:  18% (8/44)   \rUnpacking objects:  20% (9/44)   \rUnpacking objects:  22% (10/44)   \rUnpacking objects:  25% (11/44)   \rUnpacking objects:  27% (12/44)   \rUnpacking objects:  29% (13/44)   \rUnpacking objects:  31% (14/44)   \rUnpacking objects:  34% (15/44)   \rUnpacking objects:  36% (16/44)   \rUnpacking objects:  38% (17/44)   \rUnpacking objects:  40% (18/44)   \rUnpacking objects:  43% (19/44)   \rUnpacking objects:  45% (20/44)   \rUnpacking objects:  47% (21/44)   \rUnpacking objects:  50% (22/44)   \rUnpacking objects:  52% (23/44)   \rUnpacking objects:  54% (24/44)   \rUnpacking objects:  56% (25/44)   \rUnpacking objects:  59% (26/44)   \rUnpacking objects:  61% (27/44)   \rUnpacking objects:  63% (28/44)   \rUnpacking objects:  65% (29/44)   \rUnpacking objects:  68% (30/44)   \rUnpacking objects:  70% (31/44)   \rUnpacking objects:  72% (32/44)   \rUnpacking objects:  75% (33/44)   \rUnpacking objects:  77% (34/44)   \rUnpacking objects:  79% (35/44)   \rUnpacking objects:  81% (36/44)   \rUnpacking objects:  84% (37/44)   \rUnpacking objects:  86% (38/44)   \rUnpacking objects:  88% (39/44)   \rUnpacking objects:  90% (40/44)   \rUnpacking objects:  93% (41/44)   \rUnpacking objects:  95% (42/44)   \rUnpacking objects:  97% (43/44)   \rUnpacking objects: 100% (44/44)   \rUnpacking objects: 100% (44/44), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COUigBIM9kjE",
        "colab_type": "text"
      },
      "source": [
        "**Construct ANN**\n",
        "\n",
        "You first need to initialize the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlnXXK9b9vwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(state_size, 20)\n",
        "        self.layer2 = nn.Linear(20, 20)\n",
        "        self.layer3 = nn.Linear(20, 20)\n",
        "        self.action = nn.Linear(20, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        m      = torch.nn.LeakyReLU(0.1)#0.01)\n",
        "        layer1 = m(self.layer1(state))\n",
        "        layer2 =m(self.layer2(layer1))\n",
        "        layer3 = m(self.layer3(layer2))\n",
        "        action = (self.action(layer3))\n",
        "        return (action)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa_wRH3u-hVa",
        "colab_type": "text"
      },
      "source": [
        "**Generate the input-output data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySlDfk479XY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from Training_ANN_with_Pytorch.train_ann import *\n",
        "x_m = 1000  # number of samples\n",
        "x = np.random.default_rng().uniform(-5, 5, x_m)\n",
        "y = np.random.default_rng().uniform(-5, 5, x_m)\n",
        "X = np.array((x, y)).T\n",
        "\n",
        "F = []\n",
        "for i in range(0, x_m, 1):\n",
        "    f = np.array([[100 * (y[i] - x[i] ** 2) ** 2 + (1 - x[i]) ** 2, x[i]*y[i]]])\n",
        "    F.append(*f.reshape(1,2))\n",
        "F = np.array(F)\n",
        "if f.ndim==1:\n",
        "    F = F.reshape(-1, 1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC5PHsQ9-Rbh",
        "colab_type": "text"
      },
      "source": [
        "**Initialize the neural network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16iYJ1_E-LaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(X.shape[1],F.shape[1])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbiRxQTs-ZYN",
        "colab_type": "text"
      },
      "source": [
        "**Perform the training with all the default values of the class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm7gDvaT_F8v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "985684a8-220b-40e5-dbb1-0c3d7916c736"
      },
      "source": [
        "ANN = train_ann(model, X, F)\n",
        "# You can have different options to choose from lise optimizer, objective function, to plot or print\n",
        "#learning_rate = 0.01\n",
        "#ANN =  train_ann(model, X, F, optimizer=optim.Adam(model.parameters(), lr=learning_rate), \n",
        "#                 loss_fn=nn.MSELoss(), learning_rate=learning_rate, print_val=False, plot=True, validation_set=0.33, auto_normalize=True, epoch=200,batch_size=68)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1  Batches:  1  Loss:  0.416103720664978\n",
            "Epoch:  1  Batches:  2  Loss:  0.4040454626083374\n",
            "Epoch:  1  Batches:  3  Loss:  0.37699222564697266\n",
            "Epoch:  1  Batches:  4  Loss:  0.4322364926338196\n",
            "Epoch:  1  Batches:  5  Loss:  0.39534804224967957\n",
            "Epoch:  1  Batches:  6  Loss:  0.4183245003223419\n",
            "Epoch:  1  Batches:  7  Loss:  0.37718892097473145\n",
            "Epoch:  1  Batches:  8  Loss:  0.36238834261894226\n",
            "Epoch:  1  Batches:  9  Loss:  0.36856362223625183\n",
            "Epoch:  1  Batches:  10  Loss:  0.37168699502944946\n",
            "Epoch:  2  Batches:  11  Loss:  0.33763259649276733\n",
            "Epoch:  2  Batches:  12  Loss:  0.35152050852775574\n",
            "Epoch:  2  Batches:  13  Loss:  0.3745080828666687\n",
            "Epoch:  2  Batches:  14  Loss:  0.3496077060699463\n",
            "Epoch:  2  Batches:  15  Loss:  0.34389528632164\n",
            "Epoch:  2  Batches:  16  Loss:  0.3412947356700897\n",
            "Epoch:  2  Batches:  17  Loss:  0.34951627254486084\n",
            "Epoch:  2  Batches:  18  Loss:  0.3166021704673767\n",
            "Epoch:  2  Batches:  19  Loss:  0.29571810364723206\n",
            "Epoch:  2  Batches:  20  Loss:  0.291899710893631\n",
            "Epoch:  3  Batches:  21  Loss:  0.31114330887794495\n",
            "Epoch:  3  Batches:  22  Loss:  0.30631133913993835\n",
            "Epoch:  3  Batches:  23  Loss:  0.3030214011669159\n",
            "Epoch:  3  Batches:  24  Loss:  0.2999645471572876\n",
            "Epoch:  3  Batches:  25  Loss:  0.2941659092903137\n",
            "Epoch:  3  Batches:  26  Loss:  0.2715175449848175\n",
            "Epoch:  3  Batches:  27  Loss:  0.27908602356910706\n",
            "Epoch:  3  Batches:  28  Loss:  0.2699196934700012\n",
            "Epoch:  3  Batches:  29  Loss:  0.26133424043655396\n",
            "Epoch:  3  Batches:  30  Loss:  0.24492007493972778\n",
            "Epoch:  4  Batches:  31  Loss:  0.2377263307571411\n",
            "Epoch:  4  Batches:  32  Loss:  0.2481931746006012\n",
            "Epoch:  4  Batches:  33  Loss:  0.2580097019672394\n",
            "Epoch:  4  Batches:  34  Loss:  0.2537320852279663\n",
            "Epoch:  4  Batches:  35  Loss:  0.27758312225341797\n",
            "Epoch:  4  Batches:  36  Loss:  0.21976925432682037\n",
            "Epoch:  4  Batches:  37  Loss:  0.2005811333656311\n",
            "Epoch:  4  Batches:  38  Loss:  0.22173351049423218\n",
            "Epoch:  4  Batches:  39  Loss:  0.23103483021259308\n",
            "Epoch:  4  Batches:  40  Loss:  0.21756373345851898\n",
            "Epoch:  5  Batches:  41  Loss:  0.2049475610256195\n",
            "Epoch:  5  Batches:  42  Loss:  0.18615061044692993\n",
            "Epoch:  5  Batches:  43  Loss:  0.1995643973350525\n",
            "Epoch:  5  Batches:  44  Loss:  0.20081236958503723\n",
            "Epoch:  5  Batches:  45  Loss:  0.1972675919532776\n",
            "Epoch:  5  Batches:  46  Loss:  0.20683901011943817\n",
            "Epoch:  5  Batches:  47  Loss:  0.15617024898529053\n",
            "Epoch:  5  Batches:  48  Loss:  0.15809455513954163\n",
            "Epoch:  5  Batches:  49  Loss:  0.22449086606502533\n",
            "Epoch:  5  Batches:  50  Loss:  0.20453745126724243\n",
            "Epoch:  6  Batches:  51  Loss:  0.14332327246665955\n",
            "Epoch:  6  Batches:  52  Loss:  0.15972648561000824\n",
            "Epoch:  6  Batches:  53  Loss:  0.17448657751083374\n",
            "Epoch:  6  Batches:  54  Loss:  0.15767140686511993\n",
            "Epoch:  6  Batches:  55  Loss:  0.20834805071353912\n",
            "Epoch:  6  Batches:  56  Loss:  0.1442641019821167\n",
            "Epoch:  6  Batches:  57  Loss:  0.16395938396453857\n",
            "Epoch:  6  Batches:  58  Loss:  0.15694105625152588\n",
            "Epoch:  6  Batches:  59  Loss:  0.17131154239177704\n",
            "Epoch:  6  Batches:  60  Loss:  0.1382693350315094\n",
            "Epoch:  7  Batches:  61  Loss:  0.124088354408741\n",
            "Epoch:  7  Batches:  62  Loss:  0.12900640070438385\n",
            "Epoch:  7  Batches:  63  Loss:  0.16817227005958557\n",
            "Epoch:  7  Batches:  64  Loss:  0.185055211186409\n",
            "Epoch:  7  Batches:  65  Loss:  0.1667105108499527\n",
            "Epoch:  7  Batches:  66  Loss:  0.12120088189840317\n",
            "Epoch:  7  Batches:  67  Loss:  0.16358014941215515\n",
            "Epoch:  7  Batches:  68  Loss:  0.12680213153362274\n",
            "Epoch:  7  Batches:  69  Loss:  0.1571863889694214\n",
            "Epoch:  7  Batches:  70  Loss:  0.1166766956448555\n",
            "Epoch:  8  Batches:  71  Loss:  0.12119129300117493\n",
            "Epoch:  8  Batches:  72  Loss:  0.15969881415367126\n",
            "Epoch:  8  Batches:  73  Loss:  0.1328068971633911\n",
            "Epoch:  8  Batches:  74  Loss:  0.12353774905204773\n",
            "Epoch:  8  Batches:  75  Loss:  0.11942414939403534\n",
            "Epoch:  8  Batches:  76  Loss:  0.1974608153104782\n",
            "Epoch:  8  Batches:  77  Loss:  0.1582997441291809\n",
            "Epoch:  8  Batches:  78  Loss:  0.1239103302359581\n",
            "Epoch:  8  Batches:  79  Loss:  0.13745778799057007\n",
            "Epoch:  8  Batches:  80  Loss:  0.1653851866722107\n",
            "Epoch:  9  Batches:  81  Loss:  0.13416863977909088\n",
            "Epoch:  9  Batches:  82  Loss:  0.13344340026378632\n",
            "Epoch:  9  Batches:  83  Loss:  0.11273259669542313\n",
            "Epoch:  9  Batches:  84  Loss:  0.14582085609436035\n",
            "Epoch:  9  Batches:  85  Loss:  0.15211479365825653\n",
            "Epoch:  9  Batches:  86  Loss:  0.13142725825309753\n",
            "Epoch:  9  Batches:  87  Loss:  0.17534062266349792\n",
            "Epoch:  9  Batches:  88  Loss:  0.15945933759212494\n",
            "Epoch:  9  Batches:  89  Loss:  0.130598247051239\n",
            "Epoch:  9  Batches:  90  Loss:  0.12565885484218597\n",
            "Epoch:  10  Batches:  91  Loss:  0.1595434993505478\n",
            "Epoch:  10  Batches:  92  Loss:  0.10354147851467133\n",
            "Epoch:  10  Batches:  93  Loss:  0.14452029764652252\n",
            "Epoch:  10  Batches:  94  Loss:  0.13972926139831543\n",
            "Epoch:  10  Batches:  95  Loss:  0.10277316719293594\n",
            "Epoch:  10  Batches:  96  Loss:  0.15259400010108948\n",
            "Epoch:  10  Batches:  97  Loss:  0.12662926316261292\n",
            "Epoch:  10  Batches:  98  Loss:  0.15979468822479248\n",
            "Epoch:  10  Batches:  99  Loss:  0.11445768922567368\n",
            "Epoch:  10  Batches:  100  Loss:  0.17000165581703186\n",
            "Epoch:  11  Batches:  101  Loss:  0.11394783854484558\n",
            "Epoch:  11  Batches:  102  Loss:  0.14751923084259033\n",
            "Epoch:  11  Batches:  103  Loss:  0.15150439739227295\n",
            "Epoch:  11  Batches:  104  Loss:  0.10521930456161499\n",
            "Epoch:  11  Batches:  105  Loss:  0.1608935296535492\n",
            "Epoch:  11  Batches:  106  Loss:  0.15051992237567902\n",
            "Epoch:  11  Batches:  107  Loss:  0.09715842455625534\n",
            "Epoch:  11  Batches:  108  Loss:  0.12445276230573654\n",
            "Epoch:  11  Batches:  109  Loss:  0.138837069272995\n",
            "Epoch:  11  Batches:  110  Loss:  0.15131425857543945\n",
            "Epoch:  12  Batches:  111  Loss:  0.13207517564296722\n",
            "Epoch:  12  Batches:  112  Loss:  0.12101799994707108\n",
            "Epoch:  12  Batches:  113  Loss:  0.15901678800582886\n",
            "Epoch:  12  Batches:  114  Loss:  0.10369168221950531\n",
            "Epoch:  12  Batches:  115  Loss:  0.1630549430847168\n",
            "Epoch:  12  Batches:  116  Loss:  0.12089139223098755\n",
            "Epoch:  12  Batches:  117  Loss:  0.1357285976409912\n",
            "Epoch:  12  Batches:  118  Loss:  0.08699239045381546\n",
            "Epoch:  12  Batches:  119  Loss:  0.15758681297302246\n",
            "Epoch:  12  Batches:  120  Loss:  0.12315718829631805\n",
            "Epoch:  13  Batches:  121  Loss:  0.11549955606460571\n",
            "Epoch:  13  Batches:  122  Loss:  0.1342342346906662\n",
            "Epoch:  13  Batches:  123  Loss:  0.10140072554349899\n",
            "Epoch:  13  Batches:  124  Loss:  0.11589941382408142\n",
            "Epoch:  13  Batches:  125  Loss:  0.1399935483932495\n",
            "Epoch:  13  Batches:  126  Loss:  0.1221393570303917\n",
            "Epoch:  13  Batches:  127  Loss:  0.14201095700263977\n",
            "Epoch:  13  Batches:  128  Loss:  0.15434856712818146\n",
            "Epoch:  13  Batches:  129  Loss:  0.1130109652876854\n",
            "Epoch:  13  Batches:  130  Loss:  0.1253347098827362\n",
            "Epoch:  14  Batches:  131  Loss:  0.1066851019859314\n",
            "Epoch:  14  Batches:  132  Loss:  0.11109032481908798\n",
            "Epoch:  14  Batches:  133  Loss:  0.12302353978157043\n",
            "Epoch:  14  Batches:  134  Loss:  0.129914790391922\n",
            "Epoch:  14  Batches:  135  Loss:  0.12565407156944275\n",
            "Epoch:  14  Batches:  136  Loss:  0.1127755418419838\n",
            "Epoch:  14  Batches:  137  Loss:  0.11340241134166718\n",
            "Epoch:  14  Batches:  138  Loss:  0.1263313889503479\n",
            "Epoch:  14  Batches:  139  Loss:  0.13874192535877228\n",
            "Epoch:  14  Batches:  140  Loss:  0.12063686549663544\n",
            "Epoch:  15  Batches:  141  Loss:  0.13433653116226196\n",
            "Epoch:  15  Batches:  142  Loss:  0.0916380062699318\n",
            "Epoch:  15  Batches:  143  Loss:  0.11508971452713013\n",
            "Epoch:  15  Batches:  144  Loss:  0.11476396769285202\n",
            "Epoch:  15  Batches:  145  Loss:  0.11135531216859818\n",
            "Epoch:  15  Batches:  146  Loss:  0.09206579625606537\n",
            "Epoch:  15  Batches:  147  Loss:  0.09944695979356766\n",
            "Epoch:  15  Batches:  148  Loss:  0.1345813125371933\n",
            "Epoch:  15  Batches:  149  Loss:  0.11189211159944534\n",
            "Epoch:  15  Batches:  150  Loss:  0.11471447348594666\n",
            "Epoch:  16  Batches:  151  Loss:  0.0851016417145729\n",
            "Epoch:  16  Batches:  152  Loss:  0.12221600115299225\n",
            "Epoch:  16  Batches:  153  Loss:  0.10847503691911697\n",
            "Epoch:  16  Batches:  154  Loss:  0.10290075093507767\n",
            "Epoch:  16  Batches:  155  Loss:  0.10747603327035904\n",
            "Epoch:  16  Batches:  156  Loss:  0.09487485140562057\n",
            "Epoch:  16  Batches:  157  Loss:  0.0967131033539772\n",
            "Epoch:  16  Batches:  158  Loss:  0.09284469485282898\n",
            "Epoch:  16  Batches:  159  Loss:  0.0940876379609108\n",
            "Epoch:  16  Batches:  160  Loss:  0.12795385718345642\n",
            "Epoch:  17  Batches:  161  Loss:  0.0900830626487732\n",
            "Epoch:  17  Batches:  162  Loss:  0.08790136128664017\n",
            "Epoch:  17  Batches:  163  Loss:  0.07123909890651703\n",
            "Epoch:  17  Batches:  164  Loss:  0.12110980600118637\n",
            "Epoch:  17  Batches:  165  Loss:  0.11508288234472275\n",
            "Epoch:  17  Batches:  166  Loss:  0.10743927210569382\n",
            "Epoch:  17  Batches:  167  Loss:  0.10057222843170166\n",
            "Epoch:  17  Batches:  168  Loss:  0.0894523561000824\n",
            "Epoch:  17  Batches:  169  Loss:  0.10051349550485611\n",
            "Epoch:  17  Batches:  170  Loss:  0.056254126131534576\n",
            "Epoch:  18  Batches:  171  Loss:  0.07361757755279541\n",
            "Epoch:  18  Batches:  172  Loss:  0.06882096827030182\n",
            "Epoch:  18  Batches:  173  Loss:  0.09326605498790741\n",
            "Epoch:  18  Batches:  174  Loss:  0.08575622737407684\n",
            "Epoch:  18  Batches:  175  Loss:  0.05648905411362648\n",
            "Epoch:  18  Batches:  176  Loss:  0.11043164879083633\n",
            "Epoch:  18  Batches:  177  Loss:  0.08754643052816391\n",
            "Epoch:  18  Batches:  178  Loss:  0.08133534342050552\n",
            "Epoch:  18  Batches:  179  Loss:  0.11204516887664795\n",
            "Epoch:  18  Batches:  180  Loss:  0.07420302927494049\n",
            "Epoch:  19  Batches:  181  Loss:  0.05943916738033295\n",
            "Epoch:  19  Batches:  182  Loss:  0.10044988989830017\n",
            "Epoch:  19  Batches:  183  Loss:  0.057692043483257294\n",
            "Epoch:  19  Batches:  184  Loss:  0.07680537551641464\n",
            "Epoch:  19  Batches:  185  Loss:  0.09254296123981476\n",
            "Epoch:  19  Batches:  186  Loss:  0.0659492015838623\n",
            "Epoch:  19  Batches:  187  Loss:  0.07416372746229172\n",
            "Epoch:  19  Batches:  188  Loss:  0.05200871080160141\n",
            "Epoch:  19  Batches:  189  Loss:  0.09647338092327118\n",
            "Epoch:  19  Batches:  190  Loss:  0.061063047498464584\n",
            "Epoch:  20  Batches:  191  Loss:  0.08207957446575165\n",
            "Epoch:  20  Batches:  192  Loss:  0.06405927985906601\n",
            "Epoch:  20  Batches:  193  Loss:  0.045669011771678925\n",
            "Epoch:  20  Batches:  194  Loss:  0.06571310013532639\n",
            "Epoch:  20  Batches:  195  Loss:  0.07374563068151474\n",
            "Epoch:  20  Batches:  196  Loss:  0.057030487805604935\n",
            "Epoch:  20  Batches:  197  Loss:  0.08031006902456284\n",
            "Epoch:  20  Batches:  198  Loss:  0.03599013760685921\n",
            "Epoch:  20  Batches:  199  Loss:  0.06750497967004776\n",
            "Epoch:  20  Batches:  200  Loss:  0.05742272734642029\n",
            "Epoch:  21  Batches:  201  Loss:  0.04209601879119873\n",
            "Epoch:  21  Batches:  202  Loss:  0.052371732890605927\n",
            "Epoch:  21  Batches:  203  Loss:  0.0650239959359169\n",
            "Epoch:  21  Batches:  204  Loss:  0.06371571868658066\n",
            "Epoch:  21  Batches:  205  Loss:  0.047473397105932236\n",
            "Epoch:  21  Batches:  206  Loss:  0.052961722016334534\n",
            "Epoch:  21  Batches:  207  Loss:  0.053698211908340454\n",
            "Epoch:  21  Batches:  208  Loss:  0.05638469010591507\n",
            "Epoch:  21  Batches:  209  Loss:  0.07023841142654419\n",
            "Epoch:  21  Batches:  210  Loss:  0.032733600586652756\n",
            "Epoch:  22  Batches:  211  Loss:  0.04848959669470787\n",
            "Epoch:  22  Batches:  212  Loss:  0.055986154824495316\n",
            "Epoch:  22  Batches:  213  Loss:  0.04985874146223068\n",
            "Epoch:  22  Batches:  214  Loss:  0.02792038768529892\n",
            "Epoch:  22  Batches:  215  Loss:  0.053999681025743484\n",
            "Epoch:  22  Batches:  216  Loss:  0.06109469756484032\n",
            "Epoch:  22  Batches:  217  Loss:  0.057980772107839584\n",
            "Epoch:  22  Batches:  218  Loss:  0.03491288051009178\n",
            "Epoch:  22  Batches:  219  Loss:  0.025961974635720253\n",
            "Epoch:  22  Batches:  220  Loss:  0.04813435301184654\n",
            "Epoch:  23  Batches:  221  Loss:  0.03221195563673973\n",
            "Epoch:  23  Batches:  222  Loss:  0.0318128727376461\n",
            "Epoch:  23  Batches:  223  Loss:  0.03614850714802742\n",
            "Epoch:  23  Batches:  224  Loss:  0.04658569023013115\n",
            "Epoch:  23  Batches:  225  Loss:  0.03975781798362732\n",
            "Epoch:  23  Batches:  226  Loss:  0.054800845682621\n",
            "Epoch:  23  Batches:  227  Loss:  0.04268144816160202\n",
            "Epoch:  23  Batches:  228  Loss:  0.02704789489507675\n",
            "Epoch:  23  Batches:  229  Loss:  0.048332709819078445\n",
            "Epoch:  23  Batches:  230  Loss:  0.047271888703107834\n",
            "Epoch:  24  Batches:  231  Loss:  0.033899400383234024\n",
            "Epoch:  24  Batches:  232  Loss:  0.04629127308726311\n",
            "Epoch:  24  Batches:  233  Loss:  0.04173006862401962\n",
            "Epoch:  24  Batches:  234  Loss:  0.027537450194358826\n",
            "Epoch:  24  Batches:  235  Loss:  0.026821807026863098\n",
            "Epoch:  24  Batches:  236  Loss:  0.030021298676729202\n",
            "Epoch:  24  Batches:  237  Loss:  0.03288174420595169\n",
            "Epoch:  24  Batches:  238  Loss:  0.047098882496356964\n",
            "Epoch:  24  Batches:  239  Loss:  0.03725767508149147\n",
            "Epoch:  24  Batches:  240  Loss:  0.02313467115163803\n",
            "Epoch:  25  Batches:  241  Loss:  0.021142207086086273\n",
            "Epoch:  25  Batches:  242  Loss:  0.03967469185590744\n",
            "Epoch:  25  Batches:  243  Loss:  0.03107130527496338\n",
            "Epoch:  25  Batches:  244  Loss:  0.027083884924650192\n",
            "Epoch:  25  Batches:  245  Loss:  0.030572891235351562\n",
            "Epoch:  25  Batches:  246  Loss:  0.036977414041757584\n",
            "Epoch:  25  Batches:  247  Loss:  0.022300682961940765\n",
            "Epoch:  25  Batches:  248  Loss:  0.024218156933784485\n",
            "Epoch:  25  Batches:  249  Loss:  0.021598566323518753\n",
            "Epoch:  25  Batches:  250  Loss:  0.04319849610328674\n",
            "Epoch:  26  Batches:  251  Loss:  0.015676602721214294\n",
            "Epoch:  26  Batches:  252  Loss:  0.02889130637049675\n",
            "Epoch:  26  Batches:  253  Loss:  0.02308717556297779\n",
            "Epoch:  26  Batches:  254  Loss:  0.02825002931058407\n",
            "Epoch:  26  Batches:  255  Loss:  0.01737571507692337\n",
            "Epoch:  26  Batches:  256  Loss:  0.01824348419904709\n",
            "Epoch:  26  Batches:  257  Loss:  0.02297915145754814\n",
            "Epoch:  26  Batches:  258  Loss:  0.026486137881875038\n",
            "Epoch:  26  Batches:  259  Loss:  0.029625440016388893\n",
            "Epoch:  26  Batches:  260  Loss:  0.03862091153860092\n",
            "Epoch:  27  Batches:  261  Loss:  0.02019553631544113\n",
            "Epoch:  27  Batches:  262  Loss:  0.023122766986489296\n",
            "Epoch:  27  Batches:  263  Loss:  0.034553881734609604\n",
            "Epoch:  27  Batches:  264  Loss:  0.026426156982779503\n",
            "Epoch:  27  Batches:  265  Loss:  0.024896183982491493\n",
            "Epoch:  27  Batches:  266  Loss:  0.021167241036891937\n",
            "Epoch:  27  Batches:  267  Loss:  0.01182615663856268\n",
            "Epoch:  27  Batches:  268  Loss:  0.012968489900231361\n",
            "Epoch:  27  Batches:  269  Loss:  0.015240945853292942\n",
            "Epoch:  27  Batches:  270  Loss:  0.01872323453426361\n",
            "Epoch:  28  Batches:  271  Loss:  0.018504461273550987\n",
            "Epoch:  28  Batches:  272  Loss:  0.009972375817596912\n",
            "Epoch:  28  Batches:  273  Loss:  0.01692473143339157\n",
            "Epoch:  28  Batches:  274  Loss:  0.01746472157537937\n",
            "Epoch:  28  Batches:  275  Loss:  0.026092208921909332\n",
            "Epoch:  28  Batches:  276  Loss:  0.010284298099577427\n",
            "Epoch:  28  Batches:  277  Loss:  0.02015974186360836\n",
            "Epoch:  28  Batches:  278  Loss:  0.021073296666145325\n",
            "Epoch:  28  Batches:  279  Loss:  0.01623152382671833\n",
            "Epoch:  28  Batches:  280  Loss:  0.021362684667110443\n",
            "Epoch:  29  Batches:  281  Loss:  0.013078909367322922\n",
            "Epoch:  29  Batches:  282  Loss:  0.01908804103732109\n",
            "Epoch:  29  Batches:  283  Loss:  0.016403628513216972\n",
            "Epoch:  29  Batches:  284  Loss:  0.013281385414302349\n",
            "Epoch:  29  Batches:  285  Loss:  0.009038322605192661\n",
            "Epoch:  29  Batches:  286  Loss:  0.022364364936947823\n",
            "Epoch:  29  Batches:  287  Loss:  0.01505589485168457\n",
            "Epoch:  29  Batches:  288  Loss:  0.013084657490253448\n",
            "Epoch:  29  Batches:  289  Loss:  0.02128712274134159\n",
            "Epoch:  29  Batches:  290  Loss:  0.01826782338321209\n",
            "Epoch:  30  Batches:  291  Loss:  0.021096734330058098\n",
            "Epoch:  30  Batches:  292  Loss:  0.011242663487792015\n",
            "Epoch:  30  Batches:  293  Loss:  0.01456338632851839\n",
            "Epoch:  30  Batches:  294  Loss:  0.011325203813612461\n",
            "Epoch:  30  Batches:  295  Loss:  0.020649030804634094\n",
            "Epoch:  30  Batches:  296  Loss:  0.011407339945435524\n",
            "Epoch:  30  Batches:  297  Loss:  0.015137682668864727\n",
            "Epoch:  30  Batches:  298  Loss:  0.013890635222196579\n",
            "Epoch:  30  Batches:  299  Loss:  0.010906554758548737\n",
            "Epoch:  30  Batches:  300  Loss:  0.011029047891497612\n",
            "Epoch:  31  Batches:  301  Loss:  0.012223245576024055\n",
            "Epoch:  31  Batches:  302  Loss:  0.01743425987660885\n",
            "Epoch:  31  Batches:  303  Loss:  0.012818997725844383\n",
            "Epoch:  31  Batches:  304  Loss:  0.008844627067446709\n",
            "Epoch:  31  Batches:  305  Loss:  0.011885599233210087\n",
            "Epoch:  31  Batches:  306  Loss:  0.011449570767581463\n",
            "Epoch:  31  Batches:  307  Loss:  0.01267513632774353\n",
            "Epoch:  31  Batches:  308  Loss:  0.01349843479692936\n",
            "Epoch:  31  Batches:  309  Loss:  0.010904909111559391\n",
            "Epoch:  31  Batches:  310  Loss:  0.010836287401616573\n",
            "Epoch:  32  Batches:  311  Loss:  0.009633312933146954\n",
            "Epoch:  32  Batches:  312  Loss:  0.010749979875981808\n",
            "Epoch:  32  Batches:  313  Loss:  0.008067070506513119\n",
            "Epoch:  32  Batches:  314  Loss:  0.01200261153280735\n",
            "Epoch:  32  Batches:  315  Loss:  0.010667709633708\n",
            "Epoch:  32  Batches:  316  Loss:  0.014869020320475101\n",
            "Epoch:  32  Batches:  317  Loss:  0.009927750565111637\n",
            "Epoch:  32  Batches:  318  Loss:  0.008903084322810173\n",
            "Epoch:  32  Batches:  319  Loss:  0.011110086925327778\n",
            "Epoch:  32  Batches:  320  Loss:  0.011282911524176598\n",
            "Epoch:  33  Batches:  321  Loss:  0.009644812904298306\n",
            "Epoch:  33  Batches:  322  Loss:  0.008252534084022045\n",
            "Epoch:  33  Batches:  323  Loss:  0.007938079535961151\n",
            "Epoch:  33  Batches:  324  Loss:  0.006950655486434698\n",
            "Epoch:  33  Batches:  325  Loss:  0.011263631284236908\n",
            "Epoch:  33  Batches:  326  Loss:  0.009405558928847313\n",
            "Epoch:  33  Batches:  327  Loss:  0.010420523583889008\n",
            "Epoch:  33  Batches:  328  Loss:  0.009871657006442547\n",
            "Epoch:  33  Batches:  329  Loss:  0.011560668237507343\n",
            "Epoch:  33  Batches:  330  Loss:  0.008443876169621944\n",
            "Epoch:  34  Batches:  331  Loss:  0.009150456637144089\n",
            "Epoch:  34  Batches:  332  Loss:  0.008264088071882725\n",
            "Epoch:  34  Batches:  333  Loss:  0.011505789123475552\n",
            "Epoch:  34  Batches:  334  Loss:  0.007392907049506903\n",
            "Epoch:  34  Batches:  335  Loss:  0.00935583095997572\n",
            "Epoch:  34  Batches:  336  Loss:  0.006638215389102697\n",
            "Epoch:  34  Batches:  337  Loss:  0.007300317753106356\n",
            "Epoch:  34  Batches:  338  Loss:  0.010467500425875187\n",
            "Epoch:  34  Batches:  339  Loss:  0.006363341584801674\n",
            "Epoch:  34  Batches:  340  Loss:  0.007420898415148258\n",
            "Epoch:  35  Batches:  341  Loss:  0.0075214155949652195\n",
            "Epoch:  35  Batches:  342  Loss:  0.00878029316663742\n",
            "Epoch:  35  Batches:  343  Loss:  0.00796928908675909\n",
            "Epoch:  35  Batches:  344  Loss:  0.004518208093941212\n",
            "Epoch:  35  Batches:  345  Loss:  0.01053996104747057\n",
            "Epoch:  35  Batches:  346  Loss:  0.0077972374856472015\n",
            "Epoch:  35  Batches:  347  Loss:  0.00587129732593894\n",
            "Epoch:  35  Batches:  348  Loss:  0.006299122702330351\n",
            "Epoch:  35  Batches:  349  Loss:  0.008092298172414303\n",
            "Epoch:  35  Batches:  350  Loss:  0.007817283272743225\n",
            "Epoch:  36  Batches:  351  Loss:  0.007116520311683416\n",
            "Epoch:  36  Batches:  352  Loss:  0.005228063557296991\n",
            "Epoch:  36  Batches:  353  Loss:  0.007256770972162485\n",
            "Epoch:  36  Batches:  354  Loss:  0.0051136077381670475\n",
            "Epoch:  36  Batches:  355  Loss:  0.009073495864868164\n",
            "Epoch:  36  Batches:  356  Loss:  0.003727525472640991\n",
            "Epoch:  36  Batches:  357  Loss:  0.007866859436035156\n",
            "Epoch:  36  Batches:  358  Loss:  0.005666710436344147\n",
            "Epoch:  36  Batches:  359  Loss:  0.009215830825269222\n",
            "Epoch:  36  Batches:  360  Loss:  0.008358888328075409\n",
            "Epoch:  37  Batches:  361  Loss:  0.008996916934847832\n",
            "Epoch:  37  Batches:  362  Loss:  0.005081373266875744\n",
            "Epoch:  37  Batches:  363  Loss:  0.007335023023188114\n",
            "Epoch:  37  Batches:  364  Loss:  0.005080645903944969\n",
            "Epoch:  37  Batches:  365  Loss:  0.006430541630834341\n",
            "Epoch:  37  Batches:  366  Loss:  0.005796950310468674\n",
            "Epoch:  37  Batches:  367  Loss:  0.007483122870326042\n",
            "Epoch:  37  Batches:  368  Loss:  0.004533653147518635\n",
            "Epoch:  37  Batches:  369  Loss:  0.006111999042332172\n",
            "Epoch:  37  Batches:  370  Loss:  0.006090749055147171\n",
            "Epoch:  38  Batches:  371  Loss:  0.00678202835842967\n",
            "Epoch:  38  Batches:  372  Loss:  0.003983748611062765\n",
            "Epoch:  38  Batches:  373  Loss:  0.004628252238035202\n",
            "Epoch:  38  Batches:  374  Loss:  0.005510775838047266\n",
            "Epoch:  38  Batches:  375  Loss:  0.00512042548507452\n",
            "Epoch:  38  Batches:  376  Loss:  0.005239949095994234\n",
            "Epoch:  38  Batches:  377  Loss:  0.00496476236730814\n",
            "Epoch:  38  Batches:  378  Loss:  0.00616868631914258\n",
            "Epoch:  38  Batches:  379  Loss:  0.007618228439241648\n",
            "Epoch:  38  Batches:  380  Loss:  0.005519785452634096\n",
            "Epoch:  39  Batches:  381  Loss:  0.004646630492061377\n",
            "Epoch:  39  Batches:  382  Loss:  0.006054854951798916\n",
            "Epoch:  39  Batches:  383  Loss:  0.003696164581924677\n",
            "Epoch:  39  Batches:  384  Loss:  0.005738846492022276\n",
            "Epoch:  39  Batches:  385  Loss:  0.00527150696143508\n",
            "Epoch:  39  Batches:  386  Loss:  0.0040174382738769054\n",
            "Epoch:  39  Batches:  387  Loss:  0.00587125588208437\n",
            "Epoch:  39  Batches:  388  Loss:  0.0043713971972465515\n",
            "Epoch:  39  Batches:  389  Loss:  0.004794637206941843\n",
            "Epoch:  39  Batches:  390  Loss:  0.005925711710005999\n",
            "Epoch:  40  Batches:  391  Loss:  0.004528538789600134\n",
            "Epoch:  40  Batches:  392  Loss:  0.004310569725930691\n",
            "Epoch:  40  Batches:  393  Loss:  0.005334284156560898\n",
            "Epoch:  40  Batches:  394  Loss:  0.003854899201542139\n",
            "Epoch:  40  Batches:  395  Loss:  0.004705619532614946\n",
            "Epoch:  40  Batches:  396  Loss:  0.00503861578181386\n",
            "Epoch:  40  Batches:  397  Loss:  0.003835981013253331\n",
            "Epoch:  40  Batches:  398  Loss:  0.005094734951853752\n",
            "Epoch:  40  Batches:  399  Loss:  0.003450330113992095\n",
            "Epoch:  40  Batches:  400  Loss:  0.004788523074239492\n",
            "Epoch:  41  Batches:  401  Loss:  0.004017967730760574\n",
            "Epoch:  41  Batches:  402  Loss:  0.005292318761348724\n",
            "Epoch:  41  Batches:  403  Loss:  0.003504650667309761\n",
            "Epoch:  41  Batches:  404  Loss:  0.0047650826163589954\n",
            "Epoch:  41  Batches:  405  Loss:  0.004036447498947382\n",
            "Epoch:  41  Batches:  406  Loss:  0.0027346343267709017\n",
            "Epoch:  41  Batches:  407  Loss:  0.003136272542178631\n",
            "Epoch:  41  Batches:  408  Loss:  0.00422178627923131\n",
            "Epoch:  41  Batches:  409  Loss:  0.004277909640222788\n",
            "Epoch:  41  Batches:  410  Loss:  0.004415785428136587\n",
            "Epoch:  42  Batches:  411  Loss:  0.002462477656081319\n",
            "Epoch:  42  Batches:  412  Loss:  0.004770241677761078\n",
            "Epoch:  42  Batches:  413  Loss:  0.003762343432754278\n",
            "Epoch:  42  Batches:  414  Loss:  0.0037534942384809256\n",
            "Epoch:  42  Batches:  415  Loss:  0.0034666270948946476\n",
            "Epoch:  42  Batches:  416  Loss:  0.0035282541066408157\n",
            "Epoch:  42  Batches:  417  Loss:  0.004086146596819162\n",
            "Epoch:  42  Batches:  418  Loss:  0.0032428447157144547\n",
            "Epoch:  42  Batches:  419  Loss:  0.00315455230884254\n",
            "Epoch:  42  Batches:  420  Loss:  0.00465287733823061\n",
            "Epoch:  43  Batches:  421  Loss:  0.0034465785138309\n",
            "Epoch:  43  Batches:  422  Loss:  0.003666473086923361\n",
            "Epoch:  43  Batches:  423  Loss:  0.002801980823278427\n",
            "Epoch:  43  Batches:  424  Loss:  0.0034522090572863817\n",
            "Epoch:  43  Batches:  425  Loss:  0.004062448162585497\n",
            "Epoch:  43  Batches:  426  Loss:  0.0031990851275622845\n",
            "Epoch:  43  Batches:  427  Loss:  0.002749261213466525\n",
            "Epoch:  43  Batches:  428  Loss:  0.0031885262578725815\n",
            "Epoch:  43  Batches:  429  Loss:  0.002571624470874667\n",
            "Epoch:  43  Batches:  430  Loss:  0.004257796332240105\n",
            "Epoch:  44  Batches:  431  Loss:  0.0036414165515452623\n",
            "Epoch:  44  Batches:  432  Loss:  0.003286150749772787\n",
            "Epoch:  44  Batches:  433  Loss:  0.002638984238728881\n",
            "Epoch:  44  Batches:  434  Loss:  0.0035528801381587982\n",
            "Epoch:  44  Batches:  435  Loss:  0.002682141028344631\n",
            "Epoch:  44  Batches:  436  Loss:  0.0018969351658597589\n",
            "Epoch:  44  Batches:  437  Loss:  0.0036046337336301804\n",
            "Epoch:  44  Batches:  438  Loss:  0.0031426672358065844\n",
            "Epoch:  44  Batches:  439  Loss:  0.0027335411868989468\n",
            "Epoch:  44  Batches:  440  Loss:  0.003246018197387457\n",
            "Epoch:  45  Batches:  441  Loss:  0.002246691146865487\n",
            "Epoch:  45  Batches:  442  Loss:  0.003103637369349599\n",
            "Epoch:  45  Batches:  443  Loss:  0.0024994255509227514\n",
            "Epoch:  45  Batches:  444  Loss:  0.0024827145971357822\n",
            "Epoch:  45  Batches:  445  Loss:  0.0039755115285515785\n",
            "Epoch:  45  Batches:  446  Loss:  0.002590678632259369\n",
            "Epoch:  45  Batches:  447  Loss:  0.0024171785917133093\n",
            "Epoch:  45  Batches:  448  Loss:  0.0023583590518683195\n",
            "Epoch:  45  Batches:  449  Loss:  0.0023970245383679867\n",
            "Epoch:  45  Batches:  450  Loss:  0.0034045351203531027\n",
            "Epoch:  46  Batches:  451  Loss:  0.0024911616928875446\n",
            "Epoch:  46  Batches:  452  Loss:  0.0023530058097094297\n",
            "Epoch:  46  Batches:  453  Loss:  0.004179858602583408\n",
            "Epoch:  46  Batches:  454  Loss:  0.002277337247505784\n",
            "Epoch:  46  Batches:  455  Loss:  0.003112821839749813\n",
            "Epoch:  46  Batches:  456  Loss:  0.0017317065503448248\n",
            "Epoch:  46  Batches:  457  Loss:  0.0024152311962097883\n",
            "Epoch:  46  Batches:  458  Loss:  0.0017859875224530697\n",
            "Epoch:  46  Batches:  459  Loss:  0.0022960298229008913\n",
            "Epoch:  46  Batches:  460  Loss:  0.0023427868727594614\n",
            "Epoch:  47  Batches:  461  Loss:  0.0021746326237916946\n",
            "Epoch:  47  Batches:  462  Loss:  0.00242786668241024\n",
            "Epoch:  47  Batches:  463  Loss:  0.0020425862167030573\n",
            "Epoch:  47  Batches:  464  Loss:  0.0023090967442840338\n",
            "Epoch:  47  Batches:  465  Loss:  0.003211711999028921\n",
            "Epoch:  47  Batches:  466  Loss:  0.0031230084132403135\n",
            "Epoch:  47  Batches:  467  Loss:  0.0019774860702455044\n",
            "Epoch:  47  Batches:  468  Loss:  0.002327429596334696\n",
            "Epoch:  47  Batches:  469  Loss:  0.0016477280296385288\n",
            "Epoch:  47  Batches:  470  Loss:  0.0015762940747663379\n",
            "Epoch:  48  Batches:  471  Loss:  0.0020653200335800648\n",
            "Epoch:  48  Batches:  472  Loss:  0.0015292410971596837\n",
            "Epoch:  48  Batches:  473  Loss:  0.0027113123796880245\n",
            "Epoch:  48  Batches:  474  Loss:  0.0018622276838868856\n",
            "Epoch:  48  Batches:  475  Loss:  0.002207927405834198\n",
            "Epoch:  48  Batches:  476  Loss:  0.0031617528293281794\n",
            "Epoch:  48  Batches:  477  Loss:  0.001987389987334609\n",
            "Epoch:  48  Batches:  478  Loss:  0.002008972689509392\n",
            "Epoch:  48  Batches:  479  Loss:  0.0020411997102200985\n",
            "Epoch:  48  Batches:  480  Loss:  0.002612754236906767\n",
            "Epoch:  49  Batches:  481  Loss:  0.002588061150163412\n",
            "Epoch:  49  Batches:  482  Loss:  0.0019013526616618037\n",
            "Epoch:  49  Batches:  483  Loss:  0.0016305990284308791\n",
            "Epoch:  49  Batches:  484  Loss:  0.0014137434773147106\n",
            "Epoch:  49  Batches:  485  Loss:  0.002035847632214427\n",
            "Epoch:  49  Batches:  486  Loss:  0.0015597664751112461\n",
            "Epoch:  49  Batches:  487  Loss:  0.0019893378484994173\n",
            "Epoch:  49  Batches:  488  Loss:  0.0016362812602892518\n",
            "Epoch:  49  Batches:  489  Loss:  0.002743670716881752\n",
            "Epoch:  49  Batches:  490  Loss:  0.002385079627856612\n",
            "Epoch:  50  Batches:  491  Loss:  0.0018552534747868776\n",
            "Epoch:  50  Batches:  492  Loss:  0.0020658185239881277\n",
            "Epoch:  50  Batches:  493  Loss:  0.0021243731025606394\n",
            "Epoch:  50  Batches:  494  Loss:  0.0021093450486660004\n",
            "Epoch:  50  Batches:  495  Loss:  0.0020563264843076468\n",
            "Epoch:  50  Batches:  496  Loss:  0.0015857176622375846\n",
            "Epoch:  50  Batches:  497  Loss:  0.0016253169160336256\n",
            "Epoch:  50  Batches:  498  Loss:  0.0015010300558060408\n",
            "Epoch:  50  Batches:  499  Loss:  0.0016485058004036546\n",
            "Epoch:  50  Batches:  500  Loss:  0.0031518531031906605\n",
            "Epoch:  51  Batches:  501  Loss:  0.0028252138290554285\n",
            "Epoch:  51  Batches:  502  Loss:  0.0014901356771588326\n",
            "Epoch:  51  Batches:  503  Loss:  0.0015103311743587255\n",
            "Epoch:  51  Batches:  504  Loss:  0.001608132617548108\n",
            "Epoch:  51  Batches:  505  Loss:  0.001814269577153027\n",
            "Epoch:  51  Batches:  506  Loss:  0.0015791619662195444\n",
            "Epoch:  51  Batches:  507  Loss:  0.0022279100958257914\n",
            "Epoch:  51  Batches:  508  Loss:  0.002075701020658016\n",
            "Epoch:  51  Batches:  509  Loss:  0.0013496482279151678\n",
            "Epoch:  51  Batches:  510  Loss:  0.0015766557771712542\n",
            "Epoch:  52  Batches:  511  Loss:  0.0017560290871188045\n",
            "Epoch:  52  Batches:  512  Loss:  0.0018841209821403027\n",
            "Epoch:  52  Batches:  513  Loss:  0.0016932585276663303\n",
            "Epoch:  52  Batches:  514  Loss:  0.0018178855534642935\n",
            "Epoch:  52  Batches:  515  Loss:  0.0014144032029435039\n",
            "Epoch:  52  Batches:  516  Loss:  0.001794175710529089\n",
            "Epoch:  52  Batches:  517  Loss:  0.0011742982314899564\n",
            "Epoch:  52  Batches:  518  Loss:  0.0016919705085456371\n",
            "Epoch:  52  Batches:  519  Loss:  0.0018676547333598137\n",
            "Epoch:  52  Batches:  520  Loss:  0.0015296302735805511\n",
            "Epoch:  53  Batches:  521  Loss:  0.0015178315807133913\n",
            "Epoch:  53  Batches:  522  Loss:  0.0012412710348144174\n",
            "Epoch:  53  Batches:  523  Loss:  0.0013686779420822859\n",
            "Epoch:  53  Batches:  524  Loss:  0.0013948249397799373\n",
            "Epoch:  53  Batches:  525  Loss:  0.001929206890054047\n",
            "Epoch:  53  Batches:  526  Loss:  0.0015150232939049602\n",
            "Epoch:  53  Batches:  527  Loss:  0.0009573185234330595\n",
            "Epoch:  53  Batches:  528  Loss:  0.002675953321158886\n",
            "Epoch:  53  Batches:  529  Loss:  0.0012683641398325562\n",
            "Epoch:  53  Batches:  530  Loss:  0.0019382268656045198\n",
            "Epoch:  54  Batches:  531  Loss:  0.0012201869394630194\n",
            "Epoch:  54  Batches:  532  Loss:  0.001501721329987049\n",
            "Epoch:  54  Batches:  533  Loss:  0.0017219847068190575\n",
            "Epoch:  54  Batches:  534  Loss:  0.0018532127141952515\n",
            "Epoch:  54  Batches:  535  Loss:  0.0015328380977734923\n",
            "Epoch:  54  Batches:  536  Loss:  0.0015988133382052183\n",
            "Epoch:  54  Batches:  537  Loss:  0.001197211560793221\n",
            "Epoch:  54  Batches:  538  Loss:  0.0015166137600317597\n",
            "Epoch:  54  Batches:  539  Loss:  0.0010864471551030874\n",
            "Epoch:  54  Batches:  540  Loss:  0.001667915959842503\n",
            "Epoch:  55  Batches:  541  Loss:  0.001279048970900476\n",
            "Epoch:  55  Batches:  542  Loss:  0.0011162844020873308\n",
            "Epoch:  55  Batches:  543  Loss:  0.0020221027079969645\n",
            "Epoch:  55  Batches:  544  Loss:  0.001443888177163899\n",
            "Epoch:  55  Batches:  545  Loss:  0.0013494058512151241\n",
            "Epoch:  55  Batches:  546  Loss:  0.0008616484119556844\n",
            "Epoch:  55  Batches:  547  Loss:  0.0014438896905630827\n",
            "Epoch:  55  Batches:  548  Loss:  0.0015508878277614713\n",
            "Epoch:  55  Batches:  549  Loss:  0.0013034233124926686\n",
            "Epoch:  55  Batches:  550  Loss:  0.0017867585411295295\n",
            "Epoch:  56  Batches:  551  Loss:  0.001156921498477459\n",
            "Epoch:  56  Batches:  552  Loss:  0.0011578148696571589\n",
            "Epoch:  56  Batches:  553  Loss:  0.0018430168274790049\n",
            "Epoch:  56  Batches:  554  Loss:  0.0007820720784366131\n",
            "Epoch:  56  Batches:  555  Loss:  0.0020567537285387516\n",
            "Epoch:  56  Batches:  556  Loss:  0.0013257982209324837\n",
            "Epoch:  56  Batches:  557  Loss:  0.0013318159617483616\n",
            "Epoch:  56  Batches:  558  Loss:  0.0011855843476951122\n",
            "Epoch:  56  Batches:  559  Loss:  0.0011555560631677508\n",
            "Epoch:  56  Batches:  560  Loss:  0.001207430032081902\n",
            "Epoch:  57  Batches:  561  Loss:  0.0014741570921614766\n",
            "Epoch:  57  Batches:  562  Loss:  0.0014746260130777955\n",
            "Epoch:  57  Batches:  563  Loss:  0.0010639477986842394\n",
            "Epoch:  57  Batches:  564  Loss:  0.0017441418021917343\n",
            "Epoch:  57  Batches:  565  Loss:  0.0009907283820211887\n",
            "Epoch:  57  Batches:  566  Loss:  0.0007103927782736719\n",
            "Epoch:  57  Batches:  567  Loss:  0.0014490418834611773\n",
            "Epoch:  57  Batches:  568  Loss:  0.001092874794267118\n",
            "Epoch:  57  Batches:  569  Loss:  0.0013210860779508948\n",
            "Epoch:  57  Batches:  570  Loss:  0.0010529059218242764\n",
            "Epoch:  58  Batches:  571  Loss:  0.0009490721859037876\n",
            "Epoch:  58  Batches:  572  Loss:  0.0010154907358810306\n",
            "Epoch:  58  Batches:  573  Loss:  0.0008017899235710502\n",
            "Epoch:  58  Batches:  574  Loss:  0.001540504745207727\n",
            "Epoch:  58  Batches:  575  Loss:  0.001659515080973506\n",
            "Epoch:  58  Batches:  576  Loss:  0.0008553076186217368\n",
            "Epoch:  58  Batches:  577  Loss:  0.0013206900330260396\n",
            "Epoch:  58  Batches:  578  Loss:  0.001157010206952691\n",
            "Epoch:  58  Batches:  579  Loss:  0.0012537690345197916\n",
            "Epoch:  58  Batches:  580  Loss:  0.0012004937743768096\n",
            "Epoch:  59  Batches:  581  Loss:  0.0012945379130542278\n",
            "Epoch:  59  Batches:  582  Loss:  0.0010692202486097813\n",
            "Epoch:  59  Batches:  583  Loss:  0.0009808663744479418\n",
            "Epoch:  59  Batches:  584  Loss:  0.0010251388885080814\n",
            "Epoch:  59  Batches:  585  Loss:  0.0011566334869712591\n",
            "Epoch:  59  Batches:  586  Loss:  0.0013451771810650826\n",
            "Epoch:  59  Batches:  587  Loss:  0.0015014343662187457\n",
            "Epoch:  59  Batches:  588  Loss:  0.0010770962107926607\n",
            "Epoch:  59  Batches:  589  Loss:  0.0008280621841549873\n",
            "Epoch:  59  Batches:  590  Loss:  0.0011947131715714931\n",
            "Epoch:  60  Batches:  591  Loss:  0.001340928371064365\n",
            "Epoch:  60  Batches:  592  Loss:  0.0012311870232224464\n",
            "Epoch:  60  Batches:  593  Loss:  0.0010066201211884618\n",
            "Epoch:  60  Batches:  594  Loss:  0.0008324171649292111\n",
            "Epoch:  60  Batches:  595  Loss:  0.0016153844771906734\n",
            "Epoch:  60  Batches:  596  Loss:  0.0010025420924648643\n",
            "Epoch:  60  Batches:  597  Loss:  0.0007243328727781773\n",
            "Epoch:  60  Batches:  598  Loss:  0.001236415351741016\n",
            "Epoch:  60  Batches:  599  Loss:  0.0009522983455099165\n",
            "Epoch:  60  Batches:  600  Loss:  0.0008781736833043396\n",
            "Epoch:  61  Batches:  601  Loss:  0.001000571413896978\n",
            "Epoch:  61  Batches:  602  Loss:  0.0008093095384538174\n",
            "Epoch:  61  Batches:  603  Loss:  0.0010054355952888727\n",
            "Epoch:  61  Batches:  604  Loss:  0.0012806009035557508\n",
            "Epoch:  61  Batches:  605  Loss:  0.0008956726524047554\n",
            "Epoch:  61  Batches:  606  Loss:  0.001574535621330142\n",
            "Epoch:  61  Batches:  607  Loss:  0.0014522606506943703\n",
            "Epoch:  61  Batches:  608  Loss:  0.0009316258365288377\n",
            "Epoch:  61  Batches:  609  Loss:  0.0008744082297198474\n",
            "Epoch:  61  Batches:  610  Loss:  0.0006981883780099452\n",
            "Epoch:  62  Batches:  611  Loss:  0.0011517374077811837\n",
            "Epoch:  62  Batches:  612  Loss:  0.000908419955521822\n",
            "Epoch:  62  Batches:  613  Loss:  0.0013164322590455413\n",
            "Epoch:  62  Batches:  614  Loss:  0.0008965273154899478\n",
            "Epoch:  62  Batches:  615  Loss:  0.0014199517900124192\n",
            "Epoch:  62  Batches:  616  Loss:  0.0009577882592566311\n",
            "Epoch:  62  Batches:  617  Loss:  0.00083534064469859\n",
            "Epoch:  62  Batches:  618  Loss:  0.0010227981256321073\n",
            "Epoch:  62  Batches:  619  Loss:  0.0008896655635908246\n",
            "Epoch:  62  Batches:  620  Loss:  0.0007465752423740923\n",
            "Epoch:  63  Batches:  621  Loss:  0.0011026979191228747\n",
            "Epoch:  63  Batches:  622  Loss:  0.0011185214389115572\n",
            "Epoch:  63  Batches:  623  Loss:  0.0009202485089190304\n",
            "Epoch:  63  Batches:  624  Loss:  0.0009152055718004704\n",
            "Epoch:  63  Batches:  625  Loss:  0.0006728468579240143\n",
            "Epoch:  63  Batches:  626  Loss:  0.0014113103970885277\n",
            "Epoch:  63  Batches:  627  Loss:  0.0009733880870044231\n",
            "Epoch:  63  Batches:  628  Loss:  0.0007629998144693673\n",
            "Epoch:  63  Batches:  629  Loss:  0.0007948374259285629\n",
            "Epoch:  63  Batches:  630  Loss:  0.0010156818898394704\n",
            "Epoch:  64  Batches:  631  Loss:  0.000869742245413363\n",
            "Epoch:  64  Batches:  632  Loss:  0.001258830539882183\n",
            "Epoch:  64  Batches:  633  Loss:  0.0005441782413981855\n",
            "Epoch:  64  Batches:  634  Loss:  0.0008280228939838707\n",
            "Epoch:  64  Batches:  635  Loss:  0.0007628611638210714\n",
            "Epoch:  64  Batches:  636  Loss:  0.0007209261530078948\n",
            "Epoch:  64  Batches:  637  Loss:  0.0007464690133929253\n",
            "Epoch:  64  Batches:  638  Loss:  0.0011260510655120015\n",
            "Epoch:  64  Batches:  639  Loss:  0.0013703752774745226\n",
            "Epoch:  64  Batches:  640  Loss:  0.001102469745092094\n",
            "Epoch:  65  Batches:  641  Loss:  0.0011007037246599793\n",
            "Epoch:  65  Batches:  642  Loss:  0.0009481649613007903\n",
            "Epoch:  65  Batches:  643  Loss:  0.0007715179235674441\n",
            "Epoch:  65  Batches:  644  Loss:  0.0006716951611451805\n",
            "Epoch:  65  Batches:  645  Loss:  0.0008630583761259913\n",
            "Epoch:  65  Batches:  646  Loss:  0.0008218781440518796\n",
            "Epoch:  65  Batches:  647  Loss:  0.0018855867674574256\n",
            "Epoch:  65  Batches:  648  Loss:  0.0008249818347394466\n",
            "Epoch:  65  Batches:  649  Loss:  0.0009720973903313279\n",
            "Epoch:  65  Batches:  650  Loss:  0.0007225848385132849\n",
            "Epoch:  66  Batches:  651  Loss:  0.0007344036712311208\n",
            "Epoch:  66  Batches:  652  Loss:  0.000981063349172473\n",
            "Epoch:  66  Batches:  653  Loss:  0.0011832804884761572\n",
            "Epoch:  66  Batches:  654  Loss:  0.0009629257256165147\n",
            "Epoch:  66  Batches:  655  Loss:  0.001129019889049232\n",
            "Epoch:  66  Batches:  656  Loss:  0.0008278876775875688\n",
            "Epoch:  66  Batches:  657  Loss:  0.0011084264842793345\n",
            "Epoch:  66  Batches:  658  Loss:  0.0008751787827350199\n",
            "Epoch:  66  Batches:  659  Loss:  0.0006024269969202578\n",
            "Epoch:  66  Batches:  660  Loss:  0.0008945726440288126\n",
            "Epoch:  67  Batches:  661  Loss:  0.000647364417091012\n",
            "Epoch:  67  Batches:  662  Loss:  0.0018917880952358246\n",
            "Epoch:  67  Batches:  663  Loss:  0.0008430158486589789\n",
            "Epoch:  67  Batches:  664  Loss:  0.000842608860693872\n",
            "Epoch:  67  Batches:  665  Loss:  0.0008109709015116096\n",
            "Epoch:  67  Batches:  666  Loss:  0.0006088031223043799\n",
            "Epoch:  67  Batches:  667  Loss:  0.0008475272916257381\n",
            "Epoch:  67  Batches:  668  Loss:  0.0009658682974986732\n",
            "Epoch:  67  Batches:  669  Loss:  0.0009909655200317502\n",
            "Epoch:  67  Batches:  670  Loss:  0.0006300488021224737\n",
            "Epoch:  68  Batches:  671  Loss:  0.0007044479716569185\n",
            "Epoch:  68  Batches:  672  Loss:  0.0007326784543693066\n",
            "Epoch:  68  Batches:  673  Loss:  0.0009924952173605561\n",
            "Epoch:  68  Batches:  674  Loss:  0.0008639267762191594\n",
            "Epoch:  68  Batches:  675  Loss:  0.0006775594665668905\n",
            "Epoch:  68  Batches:  676  Loss:  0.0007685472373850644\n",
            "Epoch:  68  Batches:  677  Loss:  0.000777378270868212\n",
            "Epoch:  68  Batches:  678  Loss:  0.0009893459500744939\n",
            "Epoch:  68  Batches:  679  Loss:  0.0009411522769369185\n",
            "Epoch:  68  Batches:  680  Loss:  0.0010041341884061694\n",
            "Epoch:  69  Batches:  681  Loss:  0.0007700275164097548\n",
            "Epoch:  69  Batches:  682  Loss:  0.000610405346378684\n",
            "Epoch:  69  Batches:  683  Loss:  0.0013761327136307955\n",
            "Epoch:  69  Batches:  684  Loss:  0.0008105137385427952\n",
            "Epoch:  69  Batches:  685  Loss:  0.0009160691406577826\n",
            "Epoch:  69  Batches:  686  Loss:  0.000927383080124855\n",
            "Epoch:  69  Batches:  687  Loss:  0.0009922045283019543\n",
            "Epoch:  69  Batches:  688  Loss:  0.0006752549088560045\n",
            "Epoch:  69  Batches:  689  Loss:  0.0005442816182039678\n",
            "Epoch:  69  Batches:  690  Loss:  0.0005442791152745485\n",
            "Epoch:  70  Batches:  691  Loss:  0.0010337242856621742\n",
            "Epoch:  70  Batches:  692  Loss:  0.0007800685125403106\n",
            "Epoch:  70  Batches:  693  Loss:  0.00070180743932724\n",
            "Epoch:  70  Batches:  694  Loss:  0.0007216440280899405\n",
            "Epoch:  70  Batches:  695  Loss:  0.0009030467481352389\n",
            "Epoch:  70  Batches:  696  Loss:  0.0008607577765360475\n",
            "Epoch:  70  Batches:  697  Loss:  0.0007423832430504262\n",
            "Epoch:  70  Batches:  698  Loss:  0.0008954518707469106\n",
            "Epoch:  70  Batches:  699  Loss:  0.0007951818406581879\n",
            "Epoch:  70  Batches:  700  Loss:  0.0005291859270073473\n",
            "Epoch:  71  Batches:  701  Loss:  0.0007157295476645231\n",
            "Epoch:  71  Batches:  702  Loss:  0.0006410943460650742\n",
            "Epoch:  71  Batches:  703  Loss:  0.0007789955125190318\n",
            "Epoch:  71  Batches:  704  Loss:  0.0008271837141364813\n",
            "Epoch:  71  Batches:  705  Loss:  0.000801886897534132\n",
            "Epoch:  71  Batches:  706  Loss:  0.0006830825004726648\n",
            "Epoch:  71  Batches:  707  Loss:  0.0009880533907562494\n",
            "Epoch:  71  Batches:  708  Loss:  0.0008335693273693323\n",
            "Epoch:  71  Batches:  709  Loss:  0.0006568961543962359\n",
            "Epoch:  71  Batches:  710  Loss:  0.0008342479704879224\n",
            "Epoch:  72  Batches:  711  Loss:  0.0008038822561502457\n",
            "Epoch:  72  Batches:  712  Loss:  0.0006644113454967737\n",
            "Epoch:  72  Batches:  713  Loss:  0.0007453620783053339\n",
            "Epoch:  72  Batches:  714  Loss:  0.0007446662057191133\n",
            "Epoch:  72  Batches:  715  Loss:  0.0007371645770035684\n",
            "Epoch:  72  Batches:  716  Loss:  0.0007680788403376937\n",
            "Epoch:  72  Batches:  717  Loss:  0.0005984835443086922\n",
            "Epoch:  72  Batches:  718  Loss:  0.00092347152531147\n",
            "Epoch:  72  Batches:  719  Loss:  0.0005804395186714828\n",
            "Epoch:  72  Batches:  720  Loss:  0.000934380863327533\n",
            "Epoch:  73  Batches:  721  Loss:  0.0006183679215610027\n",
            "Epoch:  73  Batches:  722  Loss:  0.0005953642539680004\n",
            "Epoch:  73  Batches:  723  Loss:  0.0008373591117560863\n",
            "Epoch:  73  Batches:  724  Loss:  0.0006274168263189495\n",
            "Epoch:  73  Batches:  725  Loss:  0.0012860506540164351\n",
            "Epoch:  73  Batches:  726  Loss:  0.00041200328269042075\n",
            "Epoch:  73  Batches:  727  Loss:  0.0007918548653833568\n",
            "Epoch:  73  Batches:  728  Loss:  0.0006392136565409601\n",
            "Epoch:  73  Batches:  729  Loss:  0.0009350725449621677\n",
            "Epoch:  73  Batches:  730  Loss:  0.0006658382480964065\n",
            "Epoch:  74  Batches:  731  Loss:  0.0008299486362375319\n",
            "Epoch:  74  Batches:  732  Loss:  0.000747462036088109\n",
            "Epoch:  74  Batches:  733  Loss:  0.0007202047272585332\n",
            "Epoch:  74  Batches:  734  Loss:  0.0005827355198562145\n",
            "Epoch:  74  Batches:  735  Loss:  0.0006322768749669194\n",
            "Epoch:  74  Batches:  736  Loss:  0.0007340925512835383\n",
            "Epoch:  74  Batches:  737  Loss:  0.0005730339325964451\n",
            "Epoch:  74  Batches:  738  Loss:  0.0008411580929532647\n",
            "Epoch:  74  Batches:  739  Loss:  0.0006580778863281012\n",
            "Epoch:  74  Batches:  740  Loss:  0.0007002805359661579\n",
            "Epoch:  75  Batches:  741  Loss:  0.0008258006419055164\n",
            "Epoch:  75  Batches:  742  Loss:  0.0007953720632940531\n",
            "Epoch:  75  Batches:  743  Loss:  0.000714680238161236\n",
            "Epoch:  75  Batches:  744  Loss:  0.000594494107645005\n",
            "Epoch:  75  Batches:  745  Loss:  0.0005304995574988425\n",
            "Epoch:  75  Batches:  746  Loss:  0.001034666202031076\n",
            "Epoch:  75  Batches:  747  Loss:  0.0006200330099090934\n",
            "Epoch:  75  Batches:  748  Loss:  0.0006418253760784864\n",
            "Epoch:  75  Batches:  749  Loss:  0.000478551461128518\n",
            "Epoch:  75  Batches:  750  Loss:  0.0006594777223654091\n",
            "Epoch:  76  Batches:  751  Loss:  0.0004871131095569581\n",
            "Epoch:  76  Batches:  752  Loss:  0.000758643785957247\n",
            "Epoch:  76  Batches:  753  Loss:  0.0005572090740315616\n",
            "Epoch:  76  Batches:  754  Loss:  0.0005282807978801429\n",
            "Epoch:  76  Batches:  755  Loss:  0.0005072319181635976\n",
            "Epoch:  76  Batches:  756  Loss:  0.0006721102981828153\n",
            "Epoch:  76  Batches:  757  Loss:  0.0009770445758476853\n",
            "Epoch:  76  Batches:  758  Loss:  0.0008507162565365434\n",
            "Epoch:  76  Batches:  759  Loss:  0.000663826591335237\n",
            "Epoch:  76  Batches:  760  Loss:  0.0007224237779155374\n",
            "Epoch:  77  Batches:  761  Loss:  0.0006201591459102929\n",
            "Epoch:  77  Batches:  762  Loss:  0.0005902178236283362\n",
            "Epoch:  77  Batches:  763  Loss:  0.00044770841486752033\n",
            "Epoch:  77  Batches:  764  Loss:  0.0006923215696588159\n",
            "Epoch:  77  Batches:  765  Loss:  0.000603759428486228\n",
            "Epoch:  77  Batches:  766  Loss:  0.0006180755444802344\n",
            "Epoch:  77  Batches:  767  Loss:  0.0005319597548805177\n",
            "Epoch:  77  Batches:  768  Loss:  0.0008550588390789926\n",
            "Epoch:  77  Batches:  769  Loss:  0.0009998570894822478\n",
            "Epoch:  77  Batches:  770  Loss:  0.0007739488501101732\n",
            "Epoch:  78  Batches:  771  Loss:  0.0008474545320495963\n",
            "Epoch:  78  Batches:  772  Loss:  0.000622836931142956\n",
            "Epoch:  78  Batches:  773  Loss:  0.0005328396218828857\n",
            "Epoch:  78  Batches:  774  Loss:  0.0005384602700360119\n",
            "Epoch:  78  Batches:  775  Loss:  0.0007866756641305983\n",
            "Epoch:  78  Batches:  776  Loss:  0.0006699827499687672\n",
            "Epoch:  78  Batches:  777  Loss:  0.0005806857370771468\n",
            "Epoch:  78  Batches:  778  Loss:  0.0007627368904650211\n",
            "Epoch:  78  Batches:  779  Loss:  0.0006070917006582022\n",
            "Epoch:  78  Batches:  780  Loss:  0.0004906097892671824\n",
            "Epoch:  79  Batches:  781  Loss:  0.0004806436481885612\n",
            "Epoch:  79  Batches:  782  Loss:  0.0005179110448807478\n",
            "Epoch:  79  Batches:  783  Loss:  0.0006091367686167359\n",
            "Epoch:  79  Batches:  784  Loss:  0.0006064214394427836\n",
            "Epoch:  79  Batches:  785  Loss:  0.0005649062222801149\n",
            "Epoch:  79  Batches:  786  Loss:  0.0007340310839936137\n",
            "Epoch:  79  Batches:  787  Loss:  0.0006229663849808276\n",
            "Epoch:  79  Batches:  788  Loss:  0.0006493573309853673\n",
            "Epoch:  79  Batches:  789  Loss:  0.0007361042662523687\n",
            "Epoch:  79  Batches:  790  Loss:  0.0009754906059242785\n",
            "Epoch:  80  Batches:  791  Loss:  0.000711754139047116\n",
            "Epoch:  80  Batches:  792  Loss:  0.0005810049478895962\n",
            "Epoch:  80  Batches:  793  Loss:  0.0004906739923171699\n",
            "Epoch:  80  Batches:  794  Loss:  0.0006298887892626226\n",
            "Epoch:  80  Batches:  795  Loss:  0.0004984727129340172\n",
            "Epoch:  80  Batches:  796  Loss:  0.000647414184641093\n",
            "Epoch:  80  Batches:  797  Loss:  0.0009611155837774277\n",
            "Epoch:  80  Batches:  798  Loss:  0.0010770586086437106\n",
            "Epoch:  80  Batches:  799  Loss:  0.0005962954601272941\n",
            "Epoch:  80  Batches:  800  Loss:  0.000569017487578094\n",
            "Epoch:  81  Batches:  801  Loss:  0.0009192320285364985\n",
            "Epoch:  81  Batches:  802  Loss:  0.0007383212796412408\n",
            "Epoch:  81  Batches:  803  Loss:  0.0005624464829452336\n",
            "Epoch:  81  Batches:  804  Loss:  0.00044130138121545315\n",
            "Epoch:  81  Batches:  805  Loss:  0.0004692207439802587\n",
            "Epoch:  81  Batches:  806  Loss:  0.0008244824130088091\n",
            "Epoch:  81  Batches:  807  Loss:  0.0006446595652960241\n",
            "Epoch:  81  Batches:  808  Loss:  0.0006709914305247366\n",
            "Epoch:  81  Batches:  809  Loss:  0.0006101098260842264\n",
            "Epoch:  81  Batches:  810  Loss:  0.000616797071415931\n",
            "Epoch:  82  Batches:  811  Loss:  0.0007145542767830193\n",
            "Epoch:  82  Batches:  812  Loss:  0.0006756943766959012\n",
            "Epoch:  82  Batches:  813  Loss:  0.00048609855002723634\n",
            "Epoch:  82  Batches:  814  Loss:  0.0005631203530356288\n",
            "Epoch:  82  Batches:  815  Loss:  0.0006583008798770607\n",
            "Epoch:  82  Batches:  816  Loss:  0.0006642141961492598\n",
            "Epoch:  82  Batches:  817  Loss:  0.00047413643915206194\n",
            "Epoch:  82  Batches:  818  Loss:  0.0006956357974559069\n",
            "Epoch:  82  Batches:  819  Loss:  0.0005172064411453903\n",
            "Epoch:  82  Batches:  820  Loss:  0.0006196711910888553\n",
            "Epoch:  83  Batches:  821  Loss:  0.0003865433391183615\n",
            "Epoch:  83  Batches:  822  Loss:  0.0005073725478723645\n",
            "Epoch:  83  Batches:  823  Loss:  0.0006491384119726717\n",
            "Epoch:  83  Batches:  824  Loss:  0.00048219095333479345\n",
            "Epoch:  83  Batches:  825  Loss:  0.0008545935852453113\n",
            "Epoch:  83  Batches:  826  Loss:  0.0006644614622928202\n",
            "Epoch:  83  Batches:  827  Loss:  0.0007236165110953152\n",
            "Epoch:  83  Batches:  828  Loss:  0.000564428570214659\n",
            "Epoch:  83  Batches:  829  Loss:  0.0007083932869136333\n",
            "Epoch:  83  Batches:  830  Loss:  0.00040735473157837987\n",
            "Epoch:  84  Batches:  831  Loss:  0.0005866762949153781\n",
            "Epoch:  84  Batches:  832  Loss:  0.0006368101458065212\n",
            "Epoch:  84  Batches:  833  Loss:  0.0006536550936289132\n",
            "Epoch:  84  Batches:  834  Loss:  0.0007347139180637896\n",
            "Epoch:  84  Batches:  835  Loss:  0.0005039666430093348\n",
            "Epoch:  84  Batches:  836  Loss:  0.0006781094707548618\n",
            "Epoch:  84  Batches:  837  Loss:  0.0005697912420146167\n",
            "Epoch:  84  Batches:  838  Loss:  0.0006595866871066391\n",
            "Epoch:  84  Batches:  839  Loss:  0.0005019423551857471\n",
            "Epoch:  84  Batches:  840  Loss:  0.00038290792144834995\n",
            "Epoch:  85  Batches:  841  Loss:  0.0007595283095724881\n",
            "Epoch:  85  Batches:  842  Loss:  0.00039958400884643197\n",
            "Epoch:  85  Batches:  843  Loss:  0.000761861156206578\n",
            "Epoch:  85  Batches:  844  Loss:  0.0006208191625773907\n",
            "Epoch:  85  Batches:  845  Loss:  0.0005057496018707752\n",
            "Epoch:  85  Batches:  846  Loss:  0.0006045927293598652\n",
            "Epoch:  85  Batches:  847  Loss:  0.0004139349330216646\n",
            "Epoch:  85  Batches:  848  Loss:  0.0005926635349169374\n",
            "Epoch:  85  Batches:  849  Loss:  0.000515606312546879\n",
            "Epoch:  85  Batches:  850  Loss:  0.0007043282384984195\n",
            "Epoch:  86  Batches:  851  Loss:  0.0006735523929819465\n",
            "Epoch:  86  Batches:  852  Loss:  0.0004784331249538809\n",
            "Epoch:  86  Batches:  853  Loss:  0.0005060932599008083\n",
            "Epoch:  86  Batches:  854  Loss:  0.0005906324367970228\n",
            "Epoch:  86  Batches:  855  Loss:  0.00046351365745067596\n",
            "Epoch:  86  Batches:  856  Loss:  0.00044007616816088557\n",
            "Epoch:  86  Batches:  857  Loss:  0.0007657704991288483\n",
            "Epoch:  86  Batches:  858  Loss:  0.000489892961923033\n",
            "Epoch:  86  Batches:  859  Loss:  0.0006181041244417429\n",
            "Epoch:  86  Batches:  860  Loss:  0.0006566796801052988\n",
            "Epoch:  87  Batches:  861  Loss:  0.00043236356577835977\n",
            "Epoch:  87  Batches:  862  Loss:  0.0005923178978264332\n",
            "Epoch:  87  Batches:  863  Loss:  0.0004370569658931345\n",
            "Epoch:  87  Batches:  864  Loss:  0.0005944414297118783\n",
            "Epoch:  87  Batches:  865  Loss:  0.0007262994186021388\n",
            "Epoch:  87  Batches:  866  Loss:  0.0006376460078172386\n",
            "Epoch:  87  Batches:  867  Loss:  0.0005058486713096499\n",
            "Epoch:  87  Batches:  868  Loss:  0.0006431915098801255\n",
            "Epoch:  87  Batches:  869  Loss:  0.00034953179419972\n",
            "Epoch:  87  Batches:  870  Loss:  0.0005848550936207175\n",
            "Epoch:  88  Batches:  871  Loss:  0.0006274867919273674\n",
            "Epoch:  88  Batches:  872  Loss:  0.0005197436548769474\n",
            "Epoch:  88  Batches:  873  Loss:  0.0006077596917748451\n",
            "Epoch:  88  Batches:  874  Loss:  0.0003999211476184428\n",
            "Epoch:  88  Batches:  875  Loss:  0.0004847939417231828\n",
            "Epoch:  88  Batches:  876  Loss:  0.0007287448970600963\n",
            "Epoch:  88  Batches:  877  Loss:  0.0006113081471994519\n",
            "Epoch:  88  Batches:  878  Loss:  0.0005198806175030768\n",
            "Epoch:  88  Batches:  879  Loss:  0.00048743857769295573\n",
            "Epoch:  88  Batches:  880  Loss:  0.0004056208417750895\n",
            "Epoch:  89  Batches:  881  Loss:  0.0004860349581576884\n",
            "Epoch:  89  Batches:  882  Loss:  0.00045203909394331276\n",
            "Epoch:  89  Batches:  883  Loss:  0.0006248378194868565\n",
            "Epoch:  89  Batches:  884  Loss:  0.0004829925892408937\n",
            "Epoch:  89  Batches:  885  Loss:  0.0005143196322023869\n",
            "Epoch:  89  Batches:  886  Loss:  0.00042543403105810285\n",
            "Epoch:  89  Batches:  887  Loss:  0.000757961708586663\n",
            "Epoch:  89  Batches:  888  Loss:  0.0005391758750192821\n",
            "Epoch:  89  Batches:  889  Loss:  0.0006047629867680371\n",
            "Epoch:  89  Batches:  890  Loss:  0.0004339023435022682\n",
            "Epoch:  90  Batches:  891  Loss:  0.0003159556945320219\n",
            "Epoch:  90  Batches:  892  Loss:  0.00044608479947783053\n",
            "Epoch:  90  Batches:  893  Loss:  0.00048609296209178865\n",
            "Epoch:  90  Batches:  894  Loss:  0.00045909371692687273\n",
            "Epoch:  90  Batches:  895  Loss:  0.0003721878747455776\n",
            "Epoch:  90  Batches:  896  Loss:  0.0004729078500531614\n",
            "Epoch:  90  Batches:  897  Loss:  0.0005573422531597316\n",
            "Epoch:  90  Batches:  898  Loss:  0.0004067027475684881\n",
            "Epoch:  90  Batches:  899  Loss:  0.0010724901221692562\n",
            "Epoch:  90  Batches:  900  Loss:  0.0007437700987793505\n",
            "Epoch:  91  Batches:  901  Loss:  0.00034893918200396\n",
            "Epoch:  91  Batches:  902  Loss:  0.0005801280494779348\n",
            "Epoch:  91  Batches:  903  Loss:  0.0006222069496288896\n",
            "Epoch:  91  Batches:  904  Loss:  0.00048372367746196687\n",
            "Epoch:  91  Batches:  905  Loss:  0.0006713769398629665\n",
            "Epoch:  91  Batches:  906  Loss:  0.0005002100369893014\n",
            "Epoch:  91  Batches:  907  Loss:  0.0006649497663602233\n",
            "Epoch:  91  Batches:  908  Loss:  0.0004669907211791724\n",
            "Epoch:  91  Batches:  909  Loss:  0.0005730385310016572\n",
            "Epoch:  91  Batches:  910  Loss:  0.0004534164327196777\n",
            "Epoch:  92  Batches:  911  Loss:  0.0004892615834251046\n",
            "Epoch:  92  Batches:  912  Loss:  0.0005989486235193908\n",
            "Epoch:  92  Batches:  913  Loss:  0.0006114556454122066\n",
            "Epoch:  92  Batches:  914  Loss:  0.0004180478281341493\n",
            "Epoch:  92  Batches:  915  Loss:  0.0004711511137429625\n",
            "Epoch:  92  Batches:  916  Loss:  0.00039997807471081614\n",
            "Epoch:  92  Batches:  917  Loss:  0.0005897058872506022\n",
            "Epoch:  92  Batches:  918  Loss:  0.0005800398648716509\n",
            "Epoch:  92  Batches:  919  Loss:  0.00063871726160869\n",
            "Epoch:  92  Batches:  920  Loss:  0.00046324339928105474\n",
            "Epoch:  93  Batches:  921  Loss:  0.0004168169398326427\n",
            "Epoch:  93  Batches:  922  Loss:  0.0003978756722062826\n",
            "Epoch:  93  Batches:  923  Loss:  0.0005997518892399967\n",
            "Epoch:  93  Batches:  924  Loss:  0.0006289781886152923\n",
            "Epoch:  93  Batches:  925  Loss:  0.0006675783079117537\n",
            "Epoch:  93  Batches:  926  Loss:  0.0005353076267056167\n",
            "Epoch:  93  Batches:  927  Loss:  0.0003693438193295151\n",
            "Epoch:  93  Batches:  928  Loss:  0.0006514228880405426\n",
            "Epoch:  93  Batches:  929  Loss:  0.0003882040618918836\n",
            "Epoch:  93  Batches:  930  Loss:  0.0006106827058829367\n",
            "Epoch:  94  Batches:  931  Loss:  0.0006414578529074788\n",
            "Epoch:  94  Batches:  932  Loss:  0.0005483391578309238\n",
            "Epoch:  94  Batches:  933  Loss:  0.000568827148526907\n",
            "Epoch:  94  Batches:  934  Loss:  0.0005150507786311209\n",
            "Epoch:  94  Batches:  935  Loss:  0.0004557172069326043\n",
            "Epoch:  94  Batches:  936  Loss:  0.0004805654752999544\n",
            "Epoch:  94  Batches:  937  Loss:  0.0004528954450506717\n",
            "Epoch:  94  Batches:  938  Loss:  0.00045328764826990664\n",
            "Epoch:  94  Batches:  939  Loss:  0.0004408135137055069\n",
            "Epoch:  94  Batches:  940  Loss:  0.0004348134098108858\n",
            "Epoch:  95  Batches:  941  Loss:  0.0004479705821722746\n",
            "Epoch:  95  Batches:  942  Loss:  0.00045972756925038993\n",
            "Epoch:  95  Batches:  943  Loss:  0.000384027196560055\n",
            "Epoch:  95  Batches:  944  Loss:  0.0004530393343884498\n",
            "Epoch:  95  Batches:  945  Loss:  0.0002919943071901798\n",
            "Epoch:  95  Batches:  946  Loss:  0.0004099247162230313\n",
            "Epoch:  95  Batches:  947  Loss:  0.0007425138028338552\n",
            "Epoch:  95  Batches:  948  Loss:  0.0007184635032899678\n",
            "Epoch:  95  Batches:  949  Loss:  0.0006092064431868494\n",
            "Epoch:  95  Batches:  950  Loss:  0.0005460953689180315\n",
            "Epoch:  96  Batches:  951  Loss:  0.0005180235020816326\n",
            "Epoch:  96  Batches:  952  Loss:  0.0005192548269405961\n",
            "Epoch:  96  Batches:  953  Loss:  0.0004068788548465818\n",
            "Epoch:  96  Batches:  954  Loss:  0.0003728595038410276\n",
            "Epoch:  96  Batches:  955  Loss:  0.0006789104663766921\n",
            "Epoch:  96  Batches:  956  Loss:  0.0004169025633018464\n",
            "Epoch:  96  Batches:  957  Loss:  0.00041263518505729735\n",
            "Epoch:  96  Batches:  958  Loss:  0.0006343102431856096\n",
            "Epoch:  96  Batches:  959  Loss:  0.0005719734472222626\n",
            "Epoch:  96  Batches:  960  Loss:  0.00041015117312781513\n",
            "Epoch:  97  Batches:  961  Loss:  0.0005279351025819778\n",
            "Epoch:  97  Batches:  962  Loss:  0.0004323063185438514\n",
            "Epoch:  97  Batches:  963  Loss:  0.00040870235534384847\n",
            "Epoch:  97  Batches:  964  Loss:  0.00044983599218539894\n",
            "Epoch:  97  Batches:  965  Loss:  0.00043504382483661175\n",
            "Epoch:  97  Batches:  966  Loss:  0.000673205591738224\n",
            "Epoch:  97  Batches:  967  Loss:  0.0003777137026190758\n",
            "Epoch:  97  Batches:  968  Loss:  0.0003960812755394727\n",
            "Epoch:  97  Batches:  969  Loss:  0.000657015829347074\n",
            "Epoch:  97  Batches:  970  Loss:  0.00043887729407288134\n",
            "Epoch:  98  Batches:  971  Loss:  0.0003228341811336577\n",
            "Epoch:  98  Batches:  972  Loss:  0.0005044787540100515\n",
            "Epoch:  98  Batches:  973  Loss:  0.00032398643088527024\n",
            "Epoch:  98  Batches:  974  Loss:  0.0004702861770056188\n",
            "Epoch:  98  Batches:  975  Loss:  0.00039438207750208676\n",
            "Epoch:  98  Batches:  976  Loss:  0.0005327091785147786\n",
            "Epoch:  98  Batches:  977  Loss:  0.0005014205235056579\n",
            "Epoch:  98  Batches:  978  Loss:  0.0005081603303551674\n",
            "Epoch:  98  Batches:  979  Loss:  0.0005723041831515729\n",
            "Epoch:  98  Batches:  980  Loss:  0.0006005528848618269\n",
            "Epoch:  99  Batches:  981  Loss:  0.00040097313467413187\n",
            "Epoch:  99  Batches:  982  Loss:  0.0006296025239862502\n",
            "Epoch:  99  Batches:  983  Loss:  0.00038532630424015224\n",
            "Epoch:  99  Batches:  984  Loss:  0.0006000730791129172\n",
            "Epoch:  99  Batches:  985  Loss:  0.0005828485009260476\n",
            "Epoch:  99  Batches:  986  Loss:  0.00043951821862719953\n",
            "Epoch:  99  Batches:  987  Loss:  0.0004102388920728117\n",
            "Epoch:  99  Batches:  988  Loss:  0.00032417650800198317\n",
            "Epoch:  99  Batches:  989  Loss:  0.00045788229908794165\n",
            "Epoch:  99  Batches:  990  Loss:  0.0005299218464642763\n",
            "Epoch:  100  Batches:  991  Loss:  0.00047695045941509306\n",
            "Epoch:  100  Batches:  992  Loss:  0.0005832862807437778\n",
            "Epoch:  100  Batches:  993  Loss:  0.0005392729071900249\n",
            "Epoch:  100  Batches:  994  Loss:  0.00039643998024985194\n",
            "Epoch:  100  Batches:  995  Loss:  0.0003674078034237027\n",
            "Epoch:  100  Batches:  996  Loss:  0.0004380216705612838\n",
            "Epoch:  100  Batches:  997  Loss:  0.0003703006950672716\n",
            "Epoch:  100  Batches:  998  Loss:  0.0004869462864007801\n",
            "Epoch:  100  Batches:  999  Loss:  0.0006661732913926244\n",
            "Epoch:  100  Batches:  1000  Loss:  0.0004217826062813401\n",
            "Epoch:  101  Batches:  1001  Loss:  0.0007261073333211243\n",
            "Epoch:  101  Batches:  1002  Loss:  0.00038213026709854603\n",
            "Epoch:  101  Batches:  1003  Loss:  0.00046184324310161173\n",
            "Epoch:  101  Batches:  1004  Loss:  0.0005160998553037643\n",
            "Epoch:  101  Batches:  1005  Loss:  0.0005757134640589356\n",
            "Epoch:  101  Batches:  1006  Loss:  0.0004553956969175488\n",
            "Epoch:  101  Batches:  1007  Loss:  0.0004462075885385275\n",
            "Epoch:  101  Batches:  1008  Loss:  0.00043524743523448706\n",
            "Epoch:  101  Batches:  1009  Loss:  0.00046088278759270906\n",
            "Epoch:  101  Batches:  1010  Loss:  0.0004625575093086809\n",
            "Epoch:  102  Batches:  1011  Loss:  0.0005306840175762773\n",
            "Epoch:  102  Batches:  1012  Loss:  0.0004164481069892645\n",
            "Epoch:  102  Batches:  1013  Loss:  0.00036359456134960055\n",
            "Epoch:  102  Batches:  1014  Loss:  0.0005096340901218355\n",
            "Epoch:  102  Batches:  1015  Loss:  0.0004627202288247645\n",
            "Epoch:  102  Batches:  1016  Loss:  0.0006030137301422656\n",
            "Epoch:  102  Batches:  1017  Loss:  0.0004704125749412924\n",
            "Epoch:  102  Batches:  1018  Loss:  0.0003928873920813203\n",
            "Epoch:  102  Batches:  1019  Loss:  0.00030029602930881083\n",
            "Epoch:  102  Batches:  1020  Loss:  0.0005144457099959254\n",
            "Epoch:  103  Batches:  1021  Loss:  0.00037528318352997303\n",
            "Epoch:  103  Batches:  1022  Loss:  0.0005729089607484639\n",
            "Epoch:  103  Batches:  1023  Loss:  0.00040181330405175686\n",
            "Epoch:  103  Batches:  1024  Loss:  0.00046295468928292394\n",
            "Epoch:  103  Batches:  1025  Loss:  0.0004268006014171988\n",
            "Epoch:  103  Batches:  1026  Loss:  0.000425476289819926\n",
            "Epoch:  103  Batches:  1027  Loss:  0.0004670639173127711\n",
            "Epoch:  103  Batches:  1028  Loss:  0.0005096211098134518\n",
            "Epoch:  103  Batches:  1029  Loss:  0.0005017175571992993\n",
            "Epoch:  103  Batches:  1030  Loss:  0.0004435157752595842\n",
            "Epoch:  104  Batches:  1031  Loss:  0.00061429338529706\n",
            "Epoch:  104  Batches:  1032  Loss:  0.00046849355567246675\n",
            "Epoch:  104  Batches:  1033  Loss:  0.0004600099637173116\n",
            "Epoch:  104  Batches:  1034  Loss:  0.00037902037729509175\n",
            "Epoch:  104  Batches:  1035  Loss:  0.000651185866445303\n",
            "Epoch:  104  Batches:  1036  Loss:  0.00032528943847864866\n",
            "Epoch:  104  Batches:  1037  Loss:  0.00042209206731058657\n",
            "Epoch:  104  Batches:  1038  Loss:  0.0003859637363348156\n",
            "Epoch:  104  Batches:  1039  Loss:  0.00032767505035735667\n",
            "Epoch:  104  Batches:  1040  Loss:  0.0004484703822527081\n",
            "Epoch:  105  Batches:  1041  Loss:  0.0005470592877827585\n",
            "Epoch:  105  Batches:  1042  Loss:  0.00048686403897590935\n",
            "Epoch:  105  Batches:  1043  Loss:  0.0005470351316034794\n",
            "Epoch:  105  Batches:  1044  Loss:  0.0003978863824158907\n",
            "Epoch:  105  Batches:  1045  Loss:  0.0004523417737800628\n",
            "Epoch:  105  Batches:  1046  Loss:  0.000318689679261297\n",
            "Epoch:  105  Batches:  1047  Loss:  0.0004146411083638668\n",
            "Epoch:  105  Batches:  1048  Loss:  0.0003566340892575681\n",
            "Epoch:  105  Batches:  1049  Loss:  0.0004226616583764553\n",
            "Epoch:  105  Batches:  1050  Loss:  0.0003645969554781914\n",
            "Epoch:  106  Batches:  1051  Loss:  0.000408315536333248\n",
            "Epoch:  106  Batches:  1052  Loss:  0.0006151986308395863\n",
            "Epoch:  106  Batches:  1053  Loss:  0.000368171778973192\n",
            "Epoch:  106  Batches:  1054  Loss:  0.0004581469402182847\n",
            "Epoch:  106  Batches:  1055  Loss:  0.00036412934423424304\n",
            "Epoch:  106  Batches:  1056  Loss:  0.00035228236811235547\n",
            "Epoch:  106  Batches:  1057  Loss:  0.0004216484085191041\n",
            "Epoch:  106  Batches:  1058  Loss:  0.0005929323378950357\n",
            "Epoch:  106  Batches:  1059  Loss:  0.00036404485581442714\n",
            "Epoch:  106  Batches:  1060  Loss:  0.00038103622500784695\n",
            "Epoch:  107  Batches:  1061  Loss:  0.0003166692622471601\n",
            "Epoch:  107  Batches:  1062  Loss:  0.00037452884134836495\n",
            "Epoch:  107  Batches:  1063  Loss:  0.0006725026178173721\n",
            "Epoch:  107  Batches:  1064  Loss:  0.0004505655379034579\n",
            "Epoch:  107  Batches:  1065  Loss:  0.00029187608743086457\n",
            "Epoch:  107  Batches:  1066  Loss:  0.0004214067885186523\n",
            "Epoch:  107  Batches:  1067  Loss:  0.0004332857206463814\n",
            "Epoch:  107  Batches:  1068  Loss:  0.0004425643419381231\n",
            "Epoch:  107  Batches:  1069  Loss:  0.0003688527795020491\n",
            "Epoch:  107  Batches:  1070  Loss:  0.000529626733623445\n",
            "Epoch:  108  Batches:  1071  Loss:  0.0005020037642680109\n",
            "Epoch:  108  Batches:  1072  Loss:  0.00035100537934340537\n",
            "Epoch:  108  Batches:  1073  Loss:  0.0004863662179559469\n",
            "Epoch:  108  Batches:  1074  Loss:  0.0004565008566714823\n",
            "Epoch:  108  Batches:  1075  Loss:  0.0003350448969285935\n",
            "Epoch:  108  Batches:  1076  Loss:  0.00046542310155928135\n",
            "Epoch:  108  Batches:  1077  Loss:  0.0003521695325616747\n",
            "Epoch:  108  Batches:  1078  Loss:  0.00034630909794941545\n",
            "Epoch:  108  Batches:  1079  Loss:  0.0004901945940218866\n",
            "Epoch:  108  Batches:  1080  Loss:  0.00037551147397607565\n",
            "Epoch:  109  Batches:  1081  Loss:  0.00031748489709571004\n",
            "Epoch:  109  Batches:  1082  Loss:  0.0006256589549593627\n",
            "Epoch:  109  Batches:  1083  Loss:  0.0003616256872192025\n",
            "Epoch:  109  Batches:  1084  Loss:  0.0004899882478639483\n",
            "Epoch:  109  Batches:  1085  Loss:  0.0003587181563489139\n",
            "Epoch:  109  Batches:  1086  Loss:  0.0004333319957368076\n",
            "Epoch:  109  Batches:  1087  Loss:  0.0002941936836577952\n",
            "Epoch:  109  Batches:  1088  Loss:  0.00044573191553354263\n",
            "Epoch:  109  Batches:  1089  Loss:  0.00047441068454645574\n",
            "Epoch:  109  Batches:  1090  Loss:  0.00046545741497538984\n",
            "Epoch:  110  Batches:  1091  Loss:  0.00034936919109895825\n",
            "Epoch:  110  Batches:  1092  Loss:  0.0004288915079087019\n",
            "Epoch:  110  Batches:  1093  Loss:  0.0003389438206795603\n",
            "Epoch:  110  Batches:  1094  Loss:  0.0003672849270515144\n",
            "Epoch:  110  Batches:  1095  Loss:  0.000534693943336606\n",
            "Epoch:  110  Batches:  1096  Loss:  0.0004677786200772971\n",
            "Epoch:  110  Batches:  1097  Loss:  0.0004422597121447325\n",
            "Epoch:  110  Batches:  1098  Loss:  0.0003882533055730164\n",
            "Epoch:  110  Batches:  1099  Loss:  0.0004235135857015848\n",
            "Epoch:  110  Batches:  1100  Loss:  0.0004726472543552518\n",
            "Epoch:  111  Batches:  1101  Loss:  0.00044534984044730663\n",
            "Epoch:  111  Batches:  1102  Loss:  0.00032361267949454486\n",
            "Epoch:  111  Batches:  1103  Loss:  0.0002830018347594887\n",
            "Epoch:  111  Batches:  1104  Loss:  0.0004164759302511811\n",
            "Epoch:  111  Batches:  1105  Loss:  0.0004104271065443754\n",
            "Epoch:  111  Batches:  1106  Loss:  0.0004567312716972083\n",
            "Epoch:  111  Batches:  1107  Loss:  0.0005543659790419042\n",
            "Epoch:  111  Batches:  1108  Loss:  0.0003539437602739781\n",
            "Epoch:  111  Batches:  1109  Loss:  0.0004111351736355573\n",
            "Epoch:  111  Batches:  1110  Loss:  0.0004620267718564719\n",
            "Epoch:  112  Batches:  1111  Loss:  0.000526740332134068\n",
            "Epoch:  112  Batches:  1112  Loss:  0.00047362694749608636\n",
            "Epoch:  112  Batches:  1113  Loss:  0.00035876117181032896\n",
            "Epoch:  112  Batches:  1114  Loss:  0.0004005472583230585\n",
            "Epoch:  112  Batches:  1115  Loss:  0.0003024257021024823\n",
            "Epoch:  112  Batches:  1116  Loss:  0.00047260677092708647\n",
            "Epoch:  112  Batches:  1117  Loss:  0.00047200184781104326\n",
            "Epoch:  112  Batches:  1118  Loss:  0.0003813527000602335\n",
            "Epoch:  112  Batches:  1119  Loss:  0.0004020084743387997\n",
            "Epoch:  112  Batches:  1120  Loss:  0.000330081966239959\n",
            "Epoch:  113  Batches:  1121  Loss:  0.0003210663562640548\n",
            "Epoch:  113  Batches:  1122  Loss:  0.0004353947297204286\n",
            "Epoch:  113  Batches:  1123  Loss:  0.0004353064578026533\n",
            "Epoch:  113  Batches:  1124  Loss:  0.0005018292577005923\n",
            "Epoch:  113  Batches:  1125  Loss:  0.0003832635411527008\n",
            "Epoch:  113  Batches:  1126  Loss:  0.0003963970229960978\n",
            "Epoch:  113  Batches:  1127  Loss:  0.0003466549969743937\n",
            "Epoch:  113  Batches:  1128  Loss:  0.00040239631198346615\n",
            "Epoch:  113  Batches:  1129  Loss:  0.0004049233684781939\n",
            "Epoch:  113  Batches:  1130  Loss:  0.00035425936221145093\n",
            "Epoch:  114  Batches:  1131  Loss:  0.00038017454789951444\n",
            "Epoch:  114  Batches:  1132  Loss:  0.00030080211581662297\n",
            "Epoch:  114  Batches:  1133  Loss:  0.00030328286811709404\n",
            "Epoch:  114  Batches:  1134  Loss:  0.0005618059076368809\n",
            "Epoch:  114  Batches:  1135  Loss:  0.00037501263432204723\n",
            "Epoch:  114  Batches:  1136  Loss:  0.0004379113670438528\n",
            "Epoch:  114  Batches:  1137  Loss:  0.0004284300375729799\n",
            "Epoch:  114  Batches:  1138  Loss:  0.0003688329306896776\n",
            "Epoch:  114  Batches:  1139  Loss:  0.0004175295471213758\n",
            "Epoch:  114  Batches:  1140  Loss:  0.00048319471534341574\n",
            "Epoch:  115  Batches:  1141  Loss:  0.0003836202959064394\n",
            "Epoch:  115  Batches:  1142  Loss:  0.00037487392546609044\n",
            "Epoch:  115  Batches:  1143  Loss:  0.0004125056730117649\n",
            "Epoch:  115  Batches:  1144  Loss:  0.0004373503616079688\n",
            "Epoch:  115  Batches:  1145  Loss:  0.0003643397067207843\n",
            "Epoch:  115  Batches:  1146  Loss:  0.000530866498593241\n",
            "Epoch:  115  Batches:  1147  Loss:  0.00028479256434366107\n",
            "Epoch:  115  Batches:  1148  Loss:  0.00031626943382434547\n",
            "Epoch:  115  Batches:  1149  Loss:  0.0004505401593632996\n",
            "Epoch:  115  Batches:  1150  Loss:  0.00034426338970661163\n",
            "Epoch:  116  Batches:  1151  Loss:  0.00035127767478115857\n",
            "Epoch:  116  Batches:  1152  Loss:  0.0003158746985718608\n",
            "Epoch:  116  Batches:  1153  Loss:  0.0004385487409308553\n",
            "Epoch:  116  Batches:  1154  Loss:  0.000432043569162488\n",
            "Epoch:  116  Batches:  1155  Loss:  0.00044914675527252257\n",
            "Epoch:  116  Batches:  1156  Loss:  0.00028548602131195366\n",
            "Epoch:  116  Batches:  1157  Loss:  0.000438860704889521\n",
            "Epoch:  116  Batches:  1158  Loss:  0.00041682898881845176\n",
            "Epoch:  116  Batches:  1159  Loss:  0.00039245039806701243\n",
            "Epoch:  116  Batches:  1160  Loss:  0.0003288389998488128\n",
            "Epoch:  117  Batches:  1161  Loss:  0.00043455982813611627\n",
            "Epoch:  117  Batches:  1162  Loss:  0.0003835123497992754\n",
            "Epoch:  117  Batches:  1163  Loss:  0.0002916142111644149\n",
            "Epoch:  117  Batches:  1164  Loss:  0.0003511649847496301\n",
            "Epoch:  117  Batches:  1165  Loss:  0.0005096323438920081\n",
            "Epoch:  117  Batches:  1166  Loss:  0.0003889535728376359\n",
            "Epoch:  117  Batches:  1167  Loss:  0.0004823849303647876\n",
            "Epoch:  117  Batches:  1168  Loss:  0.00037749731563962996\n",
            "Epoch:  117  Batches:  1169  Loss:  0.00035378229222260416\n",
            "Epoch:  117  Batches:  1170  Loss:  0.0002608232316561043\n",
            "Epoch:  118  Batches:  1171  Loss:  0.00033571640960872173\n",
            "Epoch:  118  Batches:  1172  Loss:  0.00041683579911477864\n",
            "Epoch:  118  Batches:  1173  Loss:  0.0003538382879924029\n",
            "Epoch:  118  Batches:  1174  Loss:  0.0003462767053861171\n",
            "Epoch:  118  Batches:  1175  Loss:  0.0003005050530191511\n",
            "Epoch:  118  Batches:  1176  Loss:  0.0002757081820163876\n",
            "Epoch:  118  Batches:  1177  Loss:  0.0004969513975083828\n",
            "Epoch:  118  Batches:  1178  Loss:  0.000652002461720258\n",
            "Epoch:  118  Batches:  1179  Loss:  0.0003617402398958802\n",
            "Epoch:  118  Batches:  1180  Loss:  0.00029034397448413074\n",
            "Epoch:  119  Batches:  1181  Loss:  0.00035309925442561507\n",
            "Epoch:  119  Batches:  1182  Loss:  0.00036710311542265117\n",
            "Epoch:  119  Batches:  1183  Loss:  0.00032483963877893984\n",
            "Epoch:  119  Batches:  1184  Loss:  0.0004291713994462043\n",
            "Epoch:  119  Batches:  1185  Loss:  0.0004346678324509412\n",
            "Epoch:  119  Batches:  1186  Loss:  0.0004229880287311971\n",
            "Epoch:  119  Batches:  1187  Loss:  0.000304440560285002\n",
            "Epoch:  119  Batches:  1188  Loss:  0.000389656750485301\n",
            "Epoch:  119  Batches:  1189  Loss:  0.0004548648721538484\n",
            "Epoch:  119  Batches:  1190  Loss:  0.00035793273127637804\n",
            "Epoch:  120  Batches:  1191  Loss:  0.0003370835620444268\n",
            "Epoch:  120  Batches:  1192  Loss:  0.00032888149144127965\n",
            "Epoch:  120  Batches:  1193  Loss:  0.0004223370924592018\n",
            "Epoch:  120  Batches:  1194  Loss:  0.00036723693483509123\n",
            "Epoch:  120  Batches:  1195  Loss:  0.00029437371995300055\n",
            "Epoch:  120  Batches:  1196  Loss:  0.00039994186954572797\n",
            "Epoch:  120  Batches:  1197  Loss:  0.0003342206764500588\n",
            "Epoch:  120  Batches:  1198  Loss:  0.0004516680201049894\n",
            "Epoch:  120  Batches:  1199  Loss:  0.0003914239932782948\n",
            "Epoch:  120  Batches:  1200  Loss:  0.0003054807602893561\n",
            "Epoch:  121  Batches:  1201  Loss:  0.0003604068770073354\n",
            "Epoch:  121  Batches:  1202  Loss:  0.0004410407564137131\n",
            "Epoch:  121  Batches:  1203  Loss:  0.000344521104125306\n",
            "Epoch:  121  Batches:  1204  Loss:  0.00041954618063755333\n",
            "Epoch:  121  Batches:  1205  Loss:  0.0003342169802635908\n",
            "Epoch:  121  Batches:  1206  Loss:  0.0003228218120057136\n",
            "Epoch:  121  Batches:  1207  Loss:  0.0005146825569681823\n",
            "Epoch:  121  Batches:  1208  Loss:  0.000328344467561692\n",
            "Epoch:  121  Batches:  1209  Loss:  0.0003597432223614305\n",
            "Epoch:  121  Batches:  1210  Loss:  0.0002946517779491842\n",
            "Epoch:  122  Batches:  1211  Loss:  0.0003834791132248938\n",
            "Epoch:  122  Batches:  1212  Loss:  0.00048474929644726217\n",
            "Epoch:  122  Batches:  1213  Loss:  0.0003607885155361146\n",
            "Epoch:  122  Batches:  1214  Loss:  0.00035443814704194665\n",
            "Epoch:  122  Batches:  1215  Loss:  0.0002550540666561574\n",
            "Epoch:  122  Batches:  1216  Loss:  0.00031097684404812753\n",
            "Epoch:  122  Batches:  1217  Loss:  0.00045857063378207386\n",
            "Epoch:  122  Batches:  1218  Loss:  0.0004323957837186754\n",
            "Epoch:  122  Batches:  1219  Loss:  0.0002631859970279038\n",
            "Epoch:  122  Batches:  1220  Loss:  0.00043999479385092854\n",
            "Epoch:  123  Batches:  1221  Loss:  0.00032432671287097037\n",
            "Epoch:  123  Batches:  1222  Loss:  0.00039193546399474144\n",
            "Epoch:  123  Batches:  1223  Loss:  0.0003273402398917824\n",
            "Epoch:  123  Batches:  1224  Loss:  0.0003562826896086335\n",
            "Epoch:  123  Batches:  1225  Loss:  0.00043022027239203453\n",
            "Epoch:  123  Batches:  1226  Loss:  0.0004058112681377679\n",
            "Epoch:  123  Batches:  1227  Loss:  0.00044755960698239505\n",
            "Epoch:  123  Batches:  1228  Loss:  0.0003458532446529716\n",
            "Epoch:  123  Batches:  1229  Loss:  0.00038298466824926436\n",
            "Epoch:  123  Batches:  1230  Loss:  0.00034798041451722383\n",
            "Epoch:  124  Batches:  1231  Loss:  0.000360390346031636\n",
            "Epoch:  124  Batches:  1232  Loss:  0.0003034597903024405\n",
            "Epoch:  124  Batches:  1233  Loss:  0.0003587466781027615\n",
            "Epoch:  124  Batches:  1234  Loss:  0.0004877770261373371\n",
            "Epoch:  124  Batches:  1235  Loss:  0.00040637885103933513\n",
            "Epoch:  124  Batches:  1236  Loss:  0.0003743672277778387\n",
            "Epoch:  124  Batches:  1237  Loss:  0.00034167864941991866\n",
            "Epoch:  124  Batches:  1238  Loss:  0.0003443608875386417\n",
            "Epoch:  124  Batches:  1239  Loss:  0.00046393394586630166\n",
            "Epoch:  124  Batches:  1240  Loss:  0.0005076280795037746\n",
            "Epoch:  125  Batches:  1241  Loss:  0.00031798629788681865\n",
            "Epoch:  125  Batches:  1242  Loss:  0.0003747128357645124\n",
            "Epoch:  125  Batches:  1243  Loss:  0.0004157568619120866\n",
            "Epoch:  125  Batches:  1244  Loss:  0.00035282172029837966\n",
            "Epoch:  125  Batches:  1245  Loss:  0.00031607813434675336\n",
            "Epoch:  125  Batches:  1246  Loss:  0.00041068263817578554\n",
            "Epoch:  125  Batches:  1247  Loss:  0.00038242386654019356\n",
            "Epoch:  125  Batches:  1248  Loss:  0.00045480465632863343\n",
            "Epoch:  125  Batches:  1249  Loss:  0.0004206694138702005\n",
            "Epoch:  125  Batches:  1250  Loss:  0.0003809395420830697\n",
            "Epoch:  126  Batches:  1251  Loss:  0.00036094128154218197\n",
            "Epoch:  126  Batches:  1252  Loss:  0.0004925974062643945\n",
            "Epoch:  126  Batches:  1253  Loss:  0.00035734058474190533\n",
            "Epoch:  126  Batches:  1254  Loss:  0.0003575670125428587\n",
            "Epoch:  126  Batches:  1255  Loss:  0.00032883937819860876\n",
            "Epoch:  126  Batches:  1256  Loss:  0.0003279968223068863\n",
            "Epoch:  126  Batches:  1257  Loss:  0.00036606285721063614\n",
            "Epoch:  126  Batches:  1258  Loss:  0.00047377622104249895\n",
            "Epoch:  126  Batches:  1259  Loss:  0.00027033244259655476\n",
            "Epoch:  126  Batches:  1260  Loss:  0.0003133538120891899\n",
            "Epoch:  127  Batches:  1261  Loss:  0.00034783247974701226\n",
            "Epoch:  127  Batches:  1262  Loss:  0.0003768273163586855\n",
            "Epoch:  127  Batches:  1263  Loss:  0.0003918625880032778\n",
            "Epoch:  127  Batches:  1264  Loss:  0.00034416583366692066\n",
            "Epoch:  127  Batches:  1265  Loss:  0.00042811795719899237\n",
            "Epoch:  127  Batches:  1266  Loss:  0.0003096911823377013\n",
            "Epoch:  127  Batches:  1267  Loss:  0.00036311184521764517\n",
            "Epoch:  127  Batches:  1268  Loss:  0.000240250927163288\n",
            "Epoch:  127  Batches:  1269  Loss:  0.0003712741017807275\n",
            "Epoch:  127  Batches:  1270  Loss:  0.000510662270244211\n",
            "Epoch:  128  Batches:  1271  Loss:  0.0002850464079529047\n",
            "Epoch:  128  Batches:  1272  Loss:  0.00033757550409063697\n",
            "Epoch:  128  Batches:  1273  Loss:  0.0004149063606746495\n",
            "Epoch:  128  Batches:  1274  Loss:  0.0004090399597771466\n",
            "Epoch:  128  Batches:  1275  Loss:  0.00040871440432965755\n",
            "Epoch:  128  Batches:  1276  Loss:  0.00025750123313628137\n",
            "Epoch:  128  Batches:  1277  Loss:  0.00038454137393273413\n",
            "Epoch:  128  Batches:  1278  Loss:  0.00030349422013387084\n",
            "Epoch:  128  Batches:  1279  Loss:  0.0003830853383988142\n",
            "Epoch:  128  Batches:  1280  Loss:  0.0003730555472429842\n",
            "Epoch:  129  Batches:  1281  Loss:  0.00035937782377004623\n",
            "Epoch:  129  Batches:  1282  Loss:  0.00035555538488551974\n",
            "Epoch:  129  Batches:  1283  Loss:  0.0002184988115914166\n",
            "Epoch:  129  Batches:  1284  Loss:  0.00035157447564415634\n",
            "Epoch:  129  Batches:  1285  Loss:  0.00022510318376589566\n",
            "Epoch:  129  Batches:  1286  Loss:  0.0003153234429191798\n",
            "Epoch:  129  Batches:  1287  Loss:  0.0003846042964141816\n",
            "Epoch:  129  Batches:  1288  Loss:  0.0005542145227082074\n",
            "Epoch:  129  Batches:  1289  Loss:  0.00041649301419965923\n",
            "Epoch:  129  Batches:  1290  Loss:  0.000327418209053576\n",
            "Epoch:  130  Batches:  1291  Loss:  0.00032122619450092316\n",
            "Epoch:  130  Batches:  1292  Loss:  0.0003015625406987965\n",
            "Epoch:  130  Batches:  1293  Loss:  0.0003983320784755051\n",
            "Epoch:  130  Batches:  1294  Loss:  0.00035533937625586987\n",
            "Epoch:  130  Batches:  1295  Loss:  0.00040280152461491525\n",
            "Epoch:  130  Batches:  1296  Loss:  0.000293775083264336\n",
            "Epoch:  130  Batches:  1297  Loss:  0.00033161946339532733\n",
            "Epoch:  130  Batches:  1298  Loss:  0.0003357687091920525\n",
            "Epoch:  130  Batches:  1299  Loss:  0.00035186787135899067\n",
            "Epoch:  130  Batches:  1300  Loss:  0.00028482390916906297\n",
            "Epoch:  131  Batches:  1301  Loss:  0.00026920484378933907\n",
            "Epoch:  131  Batches:  1302  Loss:  0.00030954484827816486\n",
            "Epoch:  131  Batches:  1303  Loss:  0.00027055584359914064\n",
            "Epoch:  131  Batches:  1304  Loss:  0.0003792222123593092\n",
            "Epoch:  131  Batches:  1305  Loss:  0.00032906554406508803\n",
            "Epoch:  131  Batches:  1306  Loss:  0.00027473809313960373\n",
            "Epoch:  131  Batches:  1307  Loss:  0.0004924750537611544\n",
            "Epoch:  131  Batches:  1308  Loss:  0.0003231631126254797\n",
            "Epoch:  131  Batches:  1309  Loss:  0.0003838548727799207\n",
            "Epoch:  131  Batches:  1310  Loss:  0.0003485917404759675\n",
            "Epoch:  132  Batches:  1311  Loss:  0.0004409235261846334\n",
            "Epoch:  132  Batches:  1312  Loss:  0.00034274868085049093\n",
            "Epoch:  132  Batches:  1313  Loss:  0.0003464399778749794\n",
            "Epoch:  132  Batches:  1314  Loss:  0.0003175229940097779\n",
            "Epoch:  132  Batches:  1315  Loss:  0.000441523443441838\n",
            "Epoch:  132  Batches:  1316  Loss:  0.0003104499483015388\n",
            "Epoch:  132  Batches:  1317  Loss:  0.0003205605025868863\n",
            "Epoch:  132  Batches:  1318  Loss:  0.00031371376826427877\n",
            "Epoch:  132  Batches:  1319  Loss:  0.0003098360321018845\n",
            "Epoch:  132  Batches:  1320  Loss:  0.00021875395032111555\n",
            "Epoch:  133  Batches:  1321  Loss:  0.00031714834040030837\n",
            "Epoch:  133  Batches:  1322  Loss:  0.00035912051680497825\n",
            "Epoch:  133  Batches:  1323  Loss:  0.0002555225510150194\n",
            "Epoch:  133  Batches:  1324  Loss:  0.0002953415969386697\n",
            "Epoch:  133  Batches:  1325  Loss:  0.000259570952039212\n",
            "Epoch:  133  Batches:  1326  Loss:  0.00026873103342950344\n",
            "Epoch:  133  Batches:  1327  Loss:  0.00037505579530261457\n",
            "Epoch:  133  Batches:  1328  Loss:  0.0002800084766931832\n",
            "Epoch:  133  Batches:  1329  Loss:  0.00038624502485617995\n",
            "Epoch:  133  Batches:  1330  Loss:  0.00043363889562897384\n",
            "Epoch:  134  Batches:  1331  Loss:  0.00030088945641182363\n",
            "Epoch:  134  Batches:  1332  Loss:  0.0002895745274145156\n",
            "Epoch:  134  Batches:  1333  Loss:  0.0002703630307223648\n",
            "Epoch:  134  Batches:  1334  Loss:  0.0003586293023545295\n",
            "Epoch:  134  Batches:  1335  Loss:  0.00038819987094029784\n",
            "Epoch:  134  Batches:  1336  Loss:  0.0003050450759474188\n",
            "Epoch:  134  Batches:  1337  Loss:  0.0003641129587776959\n",
            "Epoch:  134  Batches:  1338  Loss:  0.0002686763182282448\n",
            "Epoch:  134  Batches:  1339  Loss:  0.00029358427855186164\n",
            "Epoch:  134  Batches:  1340  Loss:  0.0004195747314952314\n",
            "Epoch:  135  Batches:  1341  Loss:  0.00035378889879211783\n",
            "Epoch:  135  Batches:  1342  Loss:  0.0002755336172413081\n",
            "Epoch:  135  Batches:  1343  Loss:  0.00027005403535440564\n",
            "Epoch:  135  Batches:  1344  Loss:  0.00025651900796219707\n",
            "Epoch:  135  Batches:  1345  Loss:  0.00029854365857318044\n",
            "Epoch:  135  Batches:  1346  Loss:  0.0003299426753073931\n",
            "Epoch:  135  Batches:  1347  Loss:  0.0003182287618983537\n",
            "Epoch:  135  Batches:  1348  Loss:  0.0003799998667091131\n",
            "Epoch:  135  Batches:  1349  Loss:  0.00032140809344127774\n",
            "Epoch:  135  Batches:  1350  Loss:  0.0004460720520000905\n",
            "Epoch:  136  Batches:  1351  Loss:  0.00020871040760539472\n",
            "Epoch:  136  Batches:  1352  Loss:  0.0003337486123200506\n",
            "Epoch:  136  Batches:  1353  Loss:  0.00043839775025844574\n",
            "Epoch:  136  Batches:  1354  Loss:  0.00045646546641364694\n",
            "Epoch:  136  Batches:  1355  Loss:  0.00027055598911829293\n",
            "Epoch:  136  Batches:  1356  Loss:  0.000294101977488026\n",
            "Epoch:  136  Batches:  1357  Loss:  0.0003376563545316458\n",
            "Epoch:  136  Batches:  1358  Loss:  0.00022580253425985575\n",
            "Epoch:  136  Batches:  1359  Loss:  0.00028353638481348753\n",
            "Epoch:  136  Batches:  1360  Loss:  0.00037455622805282474\n",
            "Epoch:  137  Batches:  1361  Loss:  0.00027597241569310427\n",
            "Epoch:  137  Batches:  1362  Loss:  0.00033320902730338275\n",
            "Epoch:  137  Batches:  1363  Loss:  0.00023783845244906843\n",
            "Epoch:  137  Batches:  1364  Loss:  0.00025518896291032434\n",
            "Epoch:  137  Batches:  1365  Loss:  0.0003205335233360529\n",
            "Epoch:  137  Batches:  1366  Loss:  0.00034892495023086667\n",
            "Epoch:  137  Batches:  1367  Loss:  0.00032152316998690367\n",
            "Epoch:  137  Batches:  1368  Loss:  0.0002898549137171358\n",
            "Epoch:  137  Batches:  1369  Loss:  0.0003264191618654877\n",
            "Epoch:  137  Batches:  1370  Loss:  0.00035062964889220893\n",
            "Epoch:  138  Batches:  1371  Loss:  0.00035518742515705526\n",
            "Epoch:  138  Batches:  1372  Loss:  0.0003027662169188261\n",
            "Epoch:  138  Batches:  1373  Loss:  0.00029162553255446255\n",
            "Epoch:  138  Batches:  1374  Loss:  0.00030475386301986873\n",
            "Epoch:  138  Batches:  1375  Loss:  0.00022907824313733727\n",
            "Epoch:  138  Batches:  1376  Loss:  0.0004696534888353199\n",
            "Epoch:  138  Batches:  1377  Loss:  0.00028603101964108646\n",
            "Epoch:  138  Batches:  1378  Loss:  0.00026391216670162976\n",
            "Epoch:  138  Batches:  1379  Loss:  0.0003734545025508851\n",
            "Epoch:  138  Batches:  1380  Loss:  0.0003768682945519686\n",
            "Epoch:  139  Batches:  1381  Loss:  0.000312456046231091\n",
            "Epoch:  139  Batches:  1382  Loss:  0.00031361865694634616\n",
            "Epoch:  139  Batches:  1383  Loss:  0.00024942090385593474\n",
            "Epoch:  139  Batches:  1384  Loss:  0.00045724250958301127\n",
            "Epoch:  139  Batches:  1385  Loss:  0.0002954299561679363\n",
            "Epoch:  139  Batches:  1386  Loss:  0.0002780694921966642\n",
            "Epoch:  139  Batches:  1387  Loss:  0.0003100461035501212\n",
            "Epoch:  139  Batches:  1388  Loss:  0.0002970213536173105\n",
            "Epoch:  139  Batches:  1389  Loss:  0.0003708424628712237\n",
            "Epoch:  139  Batches:  1390  Loss:  0.000332229130435735\n",
            "Epoch:  140  Batches:  1391  Loss:  0.0003698306391015649\n",
            "Epoch:  140  Batches:  1392  Loss:  0.0002756583271548152\n",
            "Epoch:  140  Batches:  1393  Loss:  0.00043007530621252954\n",
            "Epoch:  140  Batches:  1394  Loss:  0.000257478270214051\n",
            "Epoch:  140  Batches:  1395  Loss:  0.0002543675946071744\n",
            "Epoch:  140  Batches:  1396  Loss:  0.0002740462659858167\n",
            "Epoch:  140  Batches:  1397  Loss:  0.00025390219525434077\n",
            "Epoch:  140  Batches:  1398  Loss:  0.00027150765527039766\n",
            "Epoch:  140  Batches:  1399  Loss:  0.0002517076791264117\n",
            "Epoch:  140  Batches:  1400  Loss:  0.0003621600044425577\n",
            "Epoch:  141  Batches:  1401  Loss:  0.00022831086243968457\n",
            "Epoch:  141  Batches:  1402  Loss:  0.000250104145379737\n",
            "Epoch:  141  Batches:  1403  Loss:  0.00038102545659057796\n",
            "Epoch:  141  Batches:  1404  Loss:  0.00033094489481300116\n",
            "Epoch:  141  Batches:  1405  Loss:  0.0002838821383193135\n",
            "Epoch:  141  Batches:  1406  Loss:  0.00037961124326102436\n",
            "Epoch:  141  Batches:  1407  Loss:  0.0003108722157776356\n",
            "Epoch:  141  Batches:  1408  Loss:  0.0003466265625320375\n",
            "Epoch:  141  Batches:  1409  Loss:  0.0003335332730785012\n",
            "Epoch:  141  Batches:  1410  Loss:  0.0002899483952205628\n",
            "Epoch:  142  Batches:  1411  Loss:  0.000361668411642313\n",
            "Epoch:  142  Batches:  1412  Loss:  0.00028225802816450596\n",
            "Epoch:  142  Batches:  1413  Loss:  0.00027520122239366174\n",
            "Epoch:  142  Batches:  1414  Loss:  0.00031299417605623603\n",
            "Epoch:  142  Batches:  1415  Loss:  0.00029411670402623713\n",
            "Epoch:  142  Batches:  1416  Loss:  0.0003107707016170025\n",
            "Epoch:  142  Batches:  1417  Loss:  0.00021600011677946895\n",
            "Epoch:  142  Batches:  1418  Loss:  0.0002894350909627974\n",
            "Epoch:  142  Batches:  1419  Loss:  0.00030177031294442713\n",
            "Epoch:  142  Batches:  1420  Loss:  0.0003201671061106026\n",
            "Epoch:  143  Batches:  1421  Loss:  0.00023334122670348734\n",
            "Epoch:  143  Batches:  1422  Loss:  0.000263157271547243\n",
            "Epoch:  143  Batches:  1423  Loss:  0.0002877085644286126\n",
            "Epoch:  143  Batches:  1424  Loss:  0.0003379014669917524\n",
            "Epoch:  143  Batches:  1425  Loss:  0.0002484627184458077\n",
            "Epoch:  143  Batches:  1426  Loss:  0.00036620767787098885\n",
            "Epoch:  143  Batches:  1427  Loss:  0.00030314180185087025\n",
            "Epoch:  143  Batches:  1428  Loss:  0.0003260170924477279\n",
            "Epoch:  143  Batches:  1429  Loss:  0.00031805099570192397\n",
            "Epoch:  143  Batches:  1430  Loss:  0.0002760197385214269\n",
            "Epoch:  144  Batches:  1431  Loss:  0.0003203184169251472\n",
            "Epoch:  144  Batches:  1432  Loss:  0.0002542635484132916\n",
            "Epoch:  144  Batches:  1433  Loss:  0.00025725385057739913\n",
            "Epoch:  144  Batches:  1434  Loss:  0.0003675416810438037\n",
            "Epoch:  144  Batches:  1435  Loss:  0.0003362004936207086\n",
            "Epoch:  144  Batches:  1436  Loss:  0.000387763197068125\n",
            "Epoch:  144  Batches:  1437  Loss:  0.00026676448760554194\n",
            "Epoch:  144  Batches:  1438  Loss:  0.0003127953677903861\n",
            "Epoch:  144  Batches:  1439  Loss:  0.00035281028249301016\n",
            "Epoch:  144  Batches:  1440  Loss:  0.00032437866320833564\n",
            "Epoch:  145  Batches:  1441  Loss:  0.00028288966859690845\n",
            "Epoch:  145  Batches:  1442  Loss:  0.00039346428820863366\n",
            "Epoch:  145  Batches:  1443  Loss:  0.000569944444578141\n",
            "Epoch:  145  Batches:  1444  Loss:  0.00021043469314463437\n",
            "Epoch:  145  Batches:  1445  Loss:  0.000316470890538767\n",
            "Epoch:  145  Batches:  1446  Loss:  0.00038007894181646407\n",
            "Epoch:  145  Batches:  1447  Loss:  0.0002596622216515243\n",
            "Epoch:  145  Batches:  1448  Loss:  0.00030137348221614957\n",
            "Epoch:  145  Batches:  1449  Loss:  0.000197459346964024\n",
            "Epoch:  145  Batches:  1450  Loss:  0.00032686968916095793\n",
            "Epoch:  146  Batches:  1451  Loss:  0.0002473308995831758\n",
            "Epoch:  146  Batches:  1452  Loss:  0.0003073133702855557\n",
            "Epoch:  146  Batches:  1453  Loss:  0.00029766137595288455\n",
            "Epoch:  146  Batches:  1454  Loss:  0.00022398572764359415\n",
            "Epoch:  146  Batches:  1455  Loss:  0.0003078548179473728\n",
            "Epoch:  146  Batches:  1456  Loss:  0.0002977869880851358\n",
            "Epoch:  146  Batches:  1457  Loss:  0.000254347687587142\n",
            "Epoch:  146  Batches:  1458  Loss:  0.00025978617486543953\n",
            "Epoch:  146  Batches:  1459  Loss:  0.00040480479947291315\n",
            "Epoch:  146  Batches:  1460  Loss:  0.00034135469468310475\n",
            "Epoch:  147  Batches:  1461  Loss:  0.0002697133459150791\n",
            "Epoch:  147  Batches:  1462  Loss:  0.0003044910845346749\n",
            "Epoch:  147  Batches:  1463  Loss:  0.00030402367701753974\n",
            "Epoch:  147  Batches:  1464  Loss:  0.0002251977421110496\n",
            "Epoch:  147  Batches:  1465  Loss:  0.00029491179157048464\n",
            "Epoch:  147  Batches:  1466  Loss:  0.0003958404704462737\n",
            "Epoch:  147  Batches:  1467  Loss:  0.0003181568463332951\n",
            "Epoch:  147  Batches:  1468  Loss:  0.00026046193670481443\n",
            "Epoch:  147  Batches:  1469  Loss:  0.0003207839617971331\n",
            "Epoch:  147  Batches:  1470  Loss:  0.00025967511464841664\n",
            "Epoch:  148  Batches:  1471  Loss:  0.0002855763887055218\n",
            "Epoch:  148  Batches:  1472  Loss:  0.00032444909447804093\n",
            "Epoch:  148  Batches:  1473  Loss:  0.0003139801847282797\n",
            "Epoch:  148  Batches:  1474  Loss:  0.0003603133955039084\n",
            "Epoch:  148  Batches:  1475  Loss:  0.00023180522839538753\n",
            "Epoch:  148  Batches:  1476  Loss:  0.00023794527805875987\n",
            "Epoch:  148  Batches:  1477  Loss:  0.00034785803291015327\n",
            "Epoch:  148  Batches:  1478  Loss:  0.00034464983036741614\n",
            "Epoch:  148  Batches:  1479  Loss:  0.00024724987451918423\n",
            "Epoch:  148  Batches:  1480  Loss:  0.0003168797993566841\n",
            "Epoch:  149  Batches:  1481  Loss:  0.00038789192331023514\n",
            "Epoch:  149  Batches:  1482  Loss:  0.00021663916413672268\n",
            "Epoch:  149  Batches:  1483  Loss:  0.0002859638771042228\n",
            "Epoch:  149  Batches:  1484  Loss:  0.0003869378415402025\n",
            "Epoch:  149  Batches:  1485  Loss:  0.00025022169575095177\n",
            "Epoch:  149  Batches:  1486  Loss:  0.0002690275141503662\n",
            "Epoch:  149  Batches:  1487  Loss:  0.00032480244408361614\n",
            "Epoch:  149  Batches:  1488  Loss:  0.00026238462305627763\n",
            "Epoch:  149  Batches:  1489  Loss:  0.0002142634621122852\n",
            "Epoch:  149  Batches:  1490  Loss:  0.0002479863178450614\n",
            "Epoch:  150  Batches:  1491  Loss:  0.00022795663971919566\n",
            "Epoch:  150  Batches:  1492  Loss:  0.00040494342101737857\n",
            "Epoch:  150  Batches:  1493  Loss:  0.00029072226607240736\n",
            "Epoch:  150  Batches:  1494  Loss:  0.0003160438791383058\n",
            "Epoch:  150  Batches:  1495  Loss:  0.00026713847182691097\n",
            "Epoch:  150  Batches:  1496  Loss:  0.00030249523115344346\n",
            "Epoch:  150  Batches:  1497  Loss:  0.00022785822511650622\n",
            "Epoch:  150  Batches:  1498  Loss:  0.00021166964143048972\n",
            "Epoch:  150  Batches:  1499  Loss:  0.00027394815697334707\n",
            "Epoch:  150  Batches:  1500  Loss:  0.0002999060379806906\n",
            "Epoch:  151  Batches:  1501  Loss:  0.00033186050131917\n",
            "Epoch:  151  Batches:  1502  Loss:  0.0002952122304122895\n",
            "Epoch:  151  Batches:  1503  Loss:  0.00033590063685551286\n",
            "Epoch:  151  Batches:  1504  Loss:  0.0002880120009649545\n",
            "Epoch:  151  Batches:  1505  Loss:  0.0002838431391865015\n",
            "Epoch:  151  Batches:  1506  Loss:  0.00020470227173063904\n",
            "Epoch:  151  Batches:  1507  Loss:  0.00026181494467891753\n",
            "Epoch:  151  Batches:  1508  Loss:  0.00029544494464062154\n",
            "Epoch:  151  Batches:  1509  Loss:  0.00027164749917574227\n",
            "Epoch:  151  Batches:  1510  Loss:  0.00024979343288578093\n",
            "Epoch:  152  Batches:  1511  Loss:  0.00021314296463970095\n",
            "Epoch:  152  Batches:  1512  Loss:  0.00024935664259828627\n",
            "Epoch:  152  Batches:  1513  Loss:  0.000261435576248914\n",
            "Epoch:  152  Batches:  1514  Loss:  0.00033211903064511716\n",
            "Epoch:  152  Batches:  1515  Loss:  0.00029087980510666966\n",
            "Epoch:  152  Batches:  1516  Loss:  0.0002550999925006181\n",
            "Epoch:  152  Batches:  1517  Loss:  0.00036804130650125444\n",
            "Epoch:  152  Batches:  1518  Loss:  0.00030321578378789127\n",
            "Epoch:  152  Batches:  1519  Loss:  0.0003480368759483099\n",
            "Epoch:  152  Batches:  1520  Loss:  0.0003653125313576311\n",
            "Epoch:  153  Batches:  1521  Loss:  0.00023737952869851142\n",
            "Epoch:  153  Batches:  1522  Loss:  0.00025888162781484425\n",
            "Epoch:  153  Batches:  1523  Loss:  0.00034863254404626787\n",
            "Epoch:  153  Batches:  1524  Loss:  0.0003215061442460865\n",
            "Epoch:  153  Batches:  1525  Loss:  0.000335017335601151\n",
            "Epoch:  153  Batches:  1526  Loss:  0.000272100733127445\n",
            "Epoch:  153  Batches:  1527  Loss:  0.00023274282284546643\n",
            "Epoch:  153  Batches:  1528  Loss:  0.000355811818735674\n",
            "Epoch:  153  Batches:  1529  Loss:  0.0003113477723672986\n",
            "Epoch:  153  Batches:  1530  Loss:  0.00027441343991085887\n",
            "Epoch:  154  Batches:  1531  Loss:  0.000311779702315107\n",
            "Epoch:  154  Batches:  1532  Loss:  0.00034788832999765873\n",
            "Epoch:  154  Batches:  1533  Loss:  0.00025234214263036847\n",
            "Epoch:  154  Batches:  1534  Loss:  0.00022928282851353288\n",
            "Epoch:  154  Batches:  1535  Loss:  0.00019637173681985587\n",
            "Epoch:  154  Batches:  1536  Loss:  0.00034566340036690235\n",
            "Epoch:  154  Batches:  1537  Loss:  0.00030810266616754234\n",
            "Epoch:  154  Batches:  1538  Loss:  0.00028995322645641863\n",
            "Epoch:  154  Batches:  1539  Loss:  0.000274782971246168\n",
            "Epoch:  154  Batches:  1540  Loss:  0.00033955503022298217\n",
            "Epoch:  155  Batches:  1541  Loss:  0.0002762724761851132\n",
            "Epoch:  155  Batches:  1542  Loss:  0.0003405450552236289\n",
            "Epoch:  155  Batches:  1543  Loss:  0.0002828805008903146\n",
            "Epoch:  155  Batches:  1544  Loss:  0.0002326317480765283\n",
            "Epoch:  155  Batches:  1545  Loss:  0.00022249323956202716\n",
            "Epoch:  155  Batches:  1546  Loss:  0.00030181222246028483\n",
            "Epoch:  155  Batches:  1547  Loss:  0.0003333641216158867\n",
            "Epoch:  155  Batches:  1548  Loss:  0.00020811738795600832\n",
            "Epoch:  155  Batches:  1549  Loss:  0.000254087004577741\n",
            "Epoch:  155  Batches:  1550  Loss:  0.0002847981231752783\n",
            "Epoch:  156  Batches:  1551  Loss:  0.00037624064134433866\n",
            "Epoch:  156  Batches:  1552  Loss:  0.0002864810230676085\n",
            "Epoch:  156  Batches:  1553  Loss:  0.00021315999038051814\n",
            "Epoch:  156  Batches:  1554  Loss:  0.00022843369515612721\n",
            "Epoch:  156  Batches:  1555  Loss:  0.0002421679237158969\n",
            "Epoch:  156  Batches:  1556  Loss:  0.000321520259603858\n",
            "Epoch:  156  Batches:  1557  Loss:  0.0002903499989770353\n",
            "Epoch:  156  Batches:  1558  Loss:  0.00028835333068855107\n",
            "Epoch:  156  Batches:  1559  Loss:  0.00023258400324266404\n",
            "Epoch:  156  Batches:  1560  Loss:  0.000284278008621186\n",
            "Epoch:  157  Batches:  1561  Loss:  0.00026980089023709297\n",
            "Epoch:  157  Batches:  1562  Loss:  0.00029953019111417234\n",
            "Epoch:  157  Batches:  1563  Loss:  0.00023213621170725673\n",
            "Epoch:  157  Batches:  1564  Loss:  0.00028447454678826034\n",
            "Epoch:  157  Batches:  1565  Loss:  0.00020933528139721602\n",
            "Epoch:  157  Batches:  1566  Loss:  0.00028077507158741355\n",
            "Epoch:  157  Batches:  1567  Loss:  0.0002507190511096269\n",
            "Epoch:  157  Batches:  1568  Loss:  0.00032077505602501333\n",
            "Epoch:  157  Batches:  1569  Loss:  0.0003303976554889232\n",
            "Epoch:  157  Batches:  1570  Loss:  0.00026808251277543604\n",
            "Epoch:  158  Batches:  1571  Loss:  0.0002632217947393656\n",
            "Epoch:  158  Batches:  1572  Loss:  0.00029875276959501207\n",
            "Epoch:  158  Batches:  1573  Loss:  0.00024695080355741084\n",
            "Epoch:  158  Batches:  1574  Loss:  0.000330370559822768\n",
            "Epoch:  158  Batches:  1575  Loss:  0.00024596418370492756\n",
            "Epoch:  158  Batches:  1576  Loss:  0.00033047524630092084\n",
            "Epoch:  158  Batches:  1577  Loss:  0.0002252214471809566\n",
            "Epoch:  158  Batches:  1578  Loss:  0.0002179694129154086\n",
            "Epoch:  158  Batches:  1579  Loss:  0.00020000025688204914\n",
            "Epoch:  158  Batches:  1580  Loss:  0.0002851675671990961\n",
            "Epoch:  159  Batches:  1581  Loss:  0.00019975101167801768\n",
            "Epoch:  159  Batches:  1582  Loss:  0.0002367511042393744\n",
            "Epoch:  159  Batches:  1583  Loss:  0.00020369277626741678\n",
            "Epoch:  159  Batches:  1584  Loss:  0.0002129891508957371\n",
            "Epoch:  159  Batches:  1585  Loss:  0.00022648034791927785\n",
            "Epoch:  159  Batches:  1586  Loss:  0.0004288004129193723\n",
            "Epoch:  159  Batches:  1587  Loss:  0.00023140588018577546\n",
            "Epoch:  159  Batches:  1588  Loss:  0.00025723528233356774\n",
            "Epoch:  159  Batches:  1589  Loss:  0.0002141167497029528\n",
            "Epoch:  159  Batches:  1590  Loss:  0.0003858568670693785\n",
            "Epoch:  160  Batches:  1591  Loss:  0.0002704160870052874\n",
            "Epoch:  160  Batches:  1592  Loss:  0.000207101707928814\n",
            "Epoch:  160  Batches:  1593  Loss:  0.0002783253730740398\n",
            "Epoch:  160  Batches:  1594  Loss:  0.0002080406411550939\n",
            "Epoch:  160  Batches:  1595  Loss:  0.0003168068651575595\n",
            "Epoch:  160  Batches:  1596  Loss:  0.0004407960514072329\n",
            "Epoch:  160  Batches:  1597  Loss:  0.00021778725204057992\n",
            "Epoch:  160  Batches:  1598  Loss:  0.00024481970467604697\n",
            "Epoch:  160  Batches:  1599  Loss:  0.00024024961749091744\n",
            "Epoch:  160  Batches:  1600  Loss:  0.0002177522546844557\n",
            "Epoch:  161  Batches:  1601  Loss:  0.00020275628776289523\n",
            "Epoch:  161  Batches:  1602  Loss:  0.00024286490224767476\n",
            "Epoch:  161  Batches:  1603  Loss:  0.00022913757129572332\n",
            "Epoch:  161  Batches:  1604  Loss:  0.00021450385975185782\n",
            "Epoch:  161  Batches:  1605  Loss:  0.0002375490585109219\n",
            "Epoch:  161  Batches:  1606  Loss:  0.0002928802277892828\n",
            "Epoch:  161  Batches:  1607  Loss:  0.00031059683533385396\n",
            "Epoch:  161  Batches:  1608  Loss:  0.00018850091146305203\n",
            "Epoch:  161  Batches:  1609  Loss:  0.00035244759055785835\n",
            "Epoch:  161  Batches:  1610  Loss:  0.00031141634099185467\n",
            "Epoch:  162  Batches:  1611  Loss:  0.0002565047761891037\n",
            "Epoch:  162  Batches:  1612  Loss:  0.00022167344286572188\n",
            "Epoch:  162  Batches:  1613  Loss:  0.0002424263657303527\n",
            "Epoch:  162  Batches:  1614  Loss:  0.0002810474543366581\n",
            "Epoch:  162  Batches:  1615  Loss:  0.00028806045884266496\n",
            "Epoch:  162  Batches:  1616  Loss:  0.0002144547615898773\n",
            "Epoch:  162  Batches:  1617  Loss:  0.0002977522381115705\n",
            "Epoch:  162  Batches:  1618  Loss:  0.0003013571258634329\n",
            "Epoch:  162  Batches:  1619  Loss:  0.0003308295854367316\n",
            "Epoch:  162  Batches:  1620  Loss:  0.00022580826771445572\n",
            "Epoch:  163  Batches:  1621  Loss:  0.0003839907585643232\n",
            "Epoch:  163  Batches:  1622  Loss:  0.0002442601544316858\n",
            "Epoch:  163  Batches:  1623  Loss:  0.00027145142666995525\n",
            "Epoch:  163  Batches:  1624  Loss:  0.0002585881156846881\n",
            "Epoch:  163  Batches:  1625  Loss:  0.0002653042902238667\n",
            "Epoch:  163  Batches:  1626  Loss:  0.0002920210827142\n",
            "Epoch:  163  Batches:  1627  Loss:  0.00025454358546994627\n",
            "Epoch:  163  Batches:  1628  Loss:  0.00020401102665346116\n",
            "Epoch:  163  Batches:  1629  Loss:  0.00019335569231770933\n",
            "Epoch:  163  Batches:  1630  Loss:  0.0002933911746367812\n",
            "Epoch:  164  Batches:  1631  Loss:  0.00027880549896508455\n",
            "Epoch:  164  Batches:  1632  Loss:  0.00030485476600006223\n",
            "Epoch:  164  Batches:  1633  Loss:  0.00019355760014150292\n",
            "Epoch:  164  Batches:  1634  Loss:  0.00021731414017267525\n",
            "Epoch:  164  Batches:  1635  Loss:  0.00030043695005588233\n",
            "Epoch:  164  Batches:  1636  Loss:  0.00023475888883695006\n",
            "Epoch:  164  Batches:  1637  Loss:  0.000282595690805465\n",
            "Epoch:  164  Batches:  1638  Loss:  0.0002209772792411968\n",
            "Epoch:  164  Batches:  1639  Loss:  0.00025152918533422053\n",
            "Epoch:  164  Batches:  1640  Loss:  0.00024221710918936878\n",
            "Epoch:  165  Batches:  1641  Loss:  0.00023240984592121094\n",
            "Epoch:  165  Batches:  1642  Loss:  0.0002187786449212581\n",
            "Epoch:  165  Batches:  1643  Loss:  0.0001790179085219279\n",
            "Epoch:  165  Batches:  1644  Loss:  0.00019781691662501544\n",
            "Epoch:  165  Batches:  1645  Loss:  0.00027202858473174274\n",
            "Epoch:  165  Batches:  1646  Loss:  0.00028732558712363243\n",
            "Epoch:  165  Batches:  1647  Loss:  0.00026380433700978756\n",
            "Epoch:  165  Batches:  1648  Loss:  0.0004175870562903583\n",
            "Epoch:  165  Batches:  1649  Loss:  0.00022427957446780056\n",
            "Epoch:  165  Batches:  1650  Loss:  0.0001796091819414869\n",
            "Epoch:  166  Batches:  1651  Loss:  0.00022961445210967213\n",
            "Epoch:  166  Batches:  1652  Loss:  0.0002777483605314046\n",
            "Epoch:  166  Batches:  1653  Loss:  0.0003755739890038967\n",
            "Epoch:  166  Batches:  1654  Loss:  0.00020922865951433778\n",
            "Epoch:  166  Batches:  1655  Loss:  0.00025112737785093486\n",
            "Epoch:  166  Batches:  1656  Loss:  0.0002135452814400196\n",
            "Epoch:  166  Batches:  1657  Loss:  0.00020969679462723434\n",
            "Epoch:  166  Batches:  1658  Loss:  0.00024974517873488367\n",
            "Epoch:  166  Batches:  1659  Loss:  0.0002349226997466758\n",
            "Epoch:  166  Batches:  1660  Loss:  0.00024717702763155103\n",
            "Epoch:  167  Batches:  1661  Loss:  0.0002492191269993782\n",
            "Epoch:  167  Batches:  1662  Loss:  0.0002325945533812046\n",
            "Epoch:  167  Batches:  1663  Loss:  0.00025309796910732985\n",
            "Epoch:  167  Batches:  1664  Loss:  0.00020222848979756236\n",
            "Epoch:  167  Batches:  1665  Loss:  0.00023479915398638695\n",
            "Epoch:  167  Batches:  1666  Loss:  0.0002871027681976557\n",
            "Epoch:  167  Batches:  1667  Loss:  0.00027302096714265645\n",
            "Epoch:  167  Batches:  1668  Loss:  0.00025902895140461624\n",
            "Epoch:  167  Batches:  1669  Loss:  0.00024253099400084466\n",
            "Epoch:  167  Batches:  1670  Loss:  0.00036044250009581447\n",
            "Epoch:  168  Batches:  1671  Loss:  0.00024141855828929693\n",
            "Epoch:  168  Batches:  1672  Loss:  0.00022039904433768243\n",
            "Epoch:  168  Batches:  1673  Loss:  0.0002789170539472252\n",
            "Epoch:  168  Batches:  1674  Loss:  0.00024266820400953293\n",
            "Epoch:  168  Batches:  1675  Loss:  0.00021141789329703897\n",
            "Epoch:  168  Batches:  1676  Loss:  0.00029007557895965874\n",
            "Epoch:  168  Batches:  1677  Loss:  0.0002582334273029119\n",
            "Epoch:  168  Batches:  1678  Loss:  0.000319006823701784\n",
            "Epoch:  168  Batches:  1679  Loss:  0.00022733815421815962\n",
            "Epoch:  168  Batches:  1680  Loss:  0.00023374269949272275\n",
            "Epoch:  169  Batches:  1681  Loss:  0.0001624627911951393\n",
            "Epoch:  169  Batches:  1682  Loss:  0.0003633796004578471\n",
            "Epoch:  169  Batches:  1683  Loss:  0.00026257536956109107\n",
            "Epoch:  169  Batches:  1684  Loss:  0.00022230332251638174\n",
            "Epoch:  169  Batches:  1685  Loss:  0.00020752778800670058\n",
            "Epoch:  169  Batches:  1686  Loss:  0.0003008177736774087\n",
            "Epoch:  169  Batches:  1687  Loss:  0.00020777674217242748\n",
            "Epoch:  169  Batches:  1688  Loss:  0.0002264771464979276\n",
            "Epoch:  169  Batches:  1689  Loss:  0.00028813499375246465\n",
            "Epoch:  169  Batches:  1690  Loss:  0.00021912781812716275\n",
            "Epoch:  170  Batches:  1691  Loss:  0.00025085220113396645\n",
            "Epoch:  170  Batches:  1692  Loss:  0.00023431386216543615\n",
            "Epoch:  170  Batches:  1693  Loss:  0.00030931472429074347\n",
            "Epoch:  170  Batches:  1694  Loss:  0.00020785655942745507\n",
            "Epoch:  170  Batches:  1695  Loss:  0.00022773952514398843\n",
            "Epoch:  170  Batches:  1696  Loss:  0.00023407516709994525\n",
            "Epoch:  170  Batches:  1697  Loss:  0.00027011928614228964\n",
            "Epoch:  170  Batches:  1698  Loss:  0.0002499437250662595\n",
            "Epoch:  170  Batches:  1699  Loss:  0.00025432949769310653\n",
            "Epoch:  170  Batches:  1700  Loss:  0.0002355041215196252\n",
            "Epoch:  171  Batches:  1701  Loss:  0.00025712756905704737\n",
            "Epoch:  171  Batches:  1702  Loss:  0.00026963790878653526\n",
            "Epoch:  171  Batches:  1703  Loss:  0.00017617264529690146\n",
            "Epoch:  171  Batches:  1704  Loss:  0.00020563637372106314\n",
            "Epoch:  171  Batches:  1705  Loss:  0.00022384326439350843\n",
            "Epoch:  171  Batches:  1706  Loss:  0.00030171332764439285\n",
            "Epoch:  171  Batches:  1707  Loss:  0.00026854302268475294\n",
            "Epoch:  171  Batches:  1708  Loss:  0.00026823108782991767\n",
            "Epoch:  171  Batches:  1709  Loss:  0.00025113936862908304\n",
            "Epoch:  171  Batches:  1710  Loss:  0.00021502588060684502\n",
            "Epoch:  172  Batches:  1711  Loss:  0.00028943322831764817\n",
            "Epoch:  172  Batches:  1712  Loss:  0.00019368872744962573\n",
            "Epoch:  172  Batches:  1713  Loss:  0.00031592126470059156\n",
            "Epoch:  172  Batches:  1714  Loss:  0.00027902089641429484\n",
            "Epoch:  172  Batches:  1715  Loss:  0.00029816871392540634\n",
            "Epoch:  172  Batches:  1716  Loss:  0.0002491016930434853\n",
            "Epoch:  172  Batches:  1717  Loss:  0.00019421096658334136\n",
            "Epoch:  172  Batches:  1718  Loss:  0.00019097926269751042\n",
            "Epoch:  172  Batches:  1719  Loss:  0.00020640381262637675\n",
            "Epoch:  172  Batches:  1720  Loss:  0.00027403366402722895\n",
            "Epoch:  173  Batches:  1721  Loss:  0.00022555726172868162\n",
            "Epoch:  173  Batches:  1722  Loss:  0.0002816879714373499\n",
            "Epoch:  173  Batches:  1723  Loss:  0.00022796116536483169\n",
            "Epoch:  173  Batches:  1724  Loss:  0.0002361672231927514\n",
            "Epoch:  173  Batches:  1725  Loss:  0.00023721379693597555\n",
            "Epoch:  173  Batches:  1726  Loss:  0.0002153068344341591\n",
            "Epoch:  173  Batches:  1727  Loss:  0.0002875315258279443\n",
            "Epoch:  173  Batches:  1728  Loss:  0.00018710538279265165\n",
            "Epoch:  173  Batches:  1729  Loss:  0.00024047179613262415\n",
            "Epoch:  173  Batches:  1730  Loss:  0.00019822583999484777\n",
            "Epoch:  174  Batches:  1731  Loss:  0.0002098727272823453\n",
            "Epoch:  174  Batches:  1732  Loss:  0.00026319653261452913\n",
            "Epoch:  174  Batches:  1733  Loss:  0.00023108994355425239\n",
            "Epoch:  174  Batches:  1734  Loss:  0.00026556121883913875\n",
            "Epoch:  174  Batches:  1735  Loss:  0.0002969364868476987\n",
            "Epoch:  174  Batches:  1736  Loss:  0.0002927964669652283\n",
            "Epoch:  174  Batches:  1737  Loss:  0.00019269698532298207\n",
            "Epoch:  174  Batches:  1738  Loss:  0.00019686952873598784\n",
            "Epoch:  174  Batches:  1739  Loss:  0.00023774418514221907\n",
            "Epoch:  174  Batches:  1740  Loss:  0.00021360922255553305\n",
            "Epoch:  175  Batches:  1741  Loss:  0.0002235971624031663\n",
            "Epoch:  175  Batches:  1742  Loss:  0.00017391990695614368\n",
            "Epoch:  175  Batches:  1743  Loss:  0.00024315633345395327\n",
            "Epoch:  175  Batches:  1744  Loss:  0.00020650282385759056\n",
            "Epoch:  175  Batches:  1745  Loss:  0.0002349966816836968\n",
            "Epoch:  175  Batches:  1746  Loss:  0.00026096220244653523\n",
            "Epoch:  175  Batches:  1747  Loss:  0.0002460325777065009\n",
            "Epoch:  175  Batches:  1748  Loss:  0.00022630745661444962\n",
            "Epoch:  175  Batches:  1749  Loss:  0.00031132096773944795\n",
            "Epoch:  175  Batches:  1750  Loss:  0.00020277721341699362\n",
            "Epoch:  176  Batches:  1751  Loss:  0.00022287263709586114\n",
            "Epoch:  176  Batches:  1752  Loss:  0.00019587775750551373\n",
            "Epoch:  176  Batches:  1753  Loss:  0.00016801251331344247\n",
            "Epoch:  176  Batches:  1754  Loss:  0.00025452947011217475\n",
            "Epoch:  176  Batches:  1755  Loss:  0.00027827845769934356\n",
            "Epoch:  176  Batches:  1756  Loss:  0.00023176772810984403\n",
            "Epoch:  176  Batches:  1757  Loss:  0.00021125920466147363\n",
            "Epoch:  176  Batches:  1758  Loss:  0.00029596436070278287\n",
            "Epoch:  176  Batches:  1759  Loss:  0.00025814809487201273\n",
            "Epoch:  176  Batches:  1760  Loss:  0.00027057292754761875\n",
            "Epoch:  177  Batches:  1761  Loss:  0.00021285968250595033\n",
            "Epoch:  177  Batches:  1762  Loss:  0.00025326063041575253\n",
            "Epoch:  177  Batches:  1763  Loss:  0.00022961365175433457\n",
            "Epoch:  177  Batches:  1764  Loss:  0.00023170759959612042\n",
            "Epoch:  177  Batches:  1765  Loss:  0.00023090587637852877\n",
            "Epoch:  177  Batches:  1766  Loss:  0.00022995642211753875\n",
            "Epoch:  177  Batches:  1767  Loss:  0.00027628030511550605\n",
            "Epoch:  177  Batches:  1768  Loss:  0.00018910516519099474\n",
            "Epoch:  177  Batches:  1769  Loss:  0.0003155827580485493\n",
            "Epoch:  177  Batches:  1770  Loss:  0.00023033973411656916\n",
            "Epoch:  178  Batches:  1771  Loss:  0.0002486425219103694\n",
            "Epoch:  178  Batches:  1772  Loss:  0.00018521373567637056\n",
            "Epoch:  178  Batches:  1773  Loss:  0.00029548327438533306\n",
            "Epoch:  178  Batches:  1774  Loss:  0.00022432836703956127\n",
            "Epoch:  178  Batches:  1775  Loss:  0.0002240116155007854\n",
            "Epoch:  178  Batches:  1776  Loss:  0.00021201565687078983\n",
            "Epoch:  178  Batches:  1777  Loss:  0.0001836452865973115\n",
            "Epoch:  178  Batches:  1778  Loss:  0.0002737069735303521\n",
            "Epoch:  178  Batches:  1779  Loss:  0.0002820054651238024\n",
            "Epoch:  178  Batches:  1780  Loss:  0.00020547810709103942\n",
            "Epoch:  179  Batches:  1781  Loss:  0.00018432457000017166\n",
            "Epoch:  179  Batches:  1782  Loss:  0.00024618333554826677\n",
            "Epoch:  179  Batches:  1783  Loss:  0.0002190013910876587\n",
            "Epoch:  179  Batches:  1784  Loss:  0.0002509647747501731\n",
            "Epoch:  179  Batches:  1785  Loss:  0.00022449306561611593\n",
            "Epoch:  179  Batches:  1786  Loss:  0.00021093306713737547\n",
            "Epoch:  179  Batches:  1787  Loss:  0.0002348526322748512\n",
            "Epoch:  179  Batches:  1788  Loss:  0.00025338580599054694\n",
            "Epoch:  179  Batches:  1789  Loss:  0.00023940969549585134\n",
            "Epoch:  179  Batches:  1790  Loss:  0.00022355980763677508\n",
            "Epoch:  180  Batches:  1791  Loss:  0.0002830230223480612\n",
            "Epoch:  180  Batches:  1792  Loss:  0.0002586376795079559\n",
            "Epoch:  180  Batches:  1793  Loss:  0.0002382441161898896\n",
            "Epoch:  180  Batches:  1794  Loss:  0.00021276471670717\n",
            "Epoch:  180  Batches:  1795  Loss:  0.0002506087766960263\n",
            "Epoch:  180  Batches:  1796  Loss:  0.00020633090753108263\n",
            "Epoch:  180  Batches:  1797  Loss:  0.0001946118427440524\n",
            "Epoch:  180  Batches:  1798  Loss:  0.0002457525988575071\n",
            "Epoch:  180  Batches:  1799  Loss:  0.00023097830126062036\n",
            "Epoch:  180  Batches:  1800  Loss:  0.0002588393690530211\n",
            "Epoch:  181  Batches:  1801  Loss:  0.00020989727636333555\n",
            "Epoch:  181  Batches:  1802  Loss:  0.00025490892585366964\n",
            "Epoch:  181  Batches:  1803  Loss:  0.00035605032462626696\n",
            "Epoch:  181  Batches:  1804  Loss:  0.00026806839741766453\n",
            "Epoch:  181  Batches:  1805  Loss:  0.00014456002099905163\n",
            "Epoch:  181  Batches:  1806  Loss:  0.00020709271484520286\n",
            "Epoch:  181  Batches:  1807  Loss:  0.00023158028488978744\n",
            "Epoch:  181  Batches:  1808  Loss:  0.0002174772380385548\n",
            "Epoch:  181  Batches:  1809  Loss:  0.0002217303990619257\n",
            "Epoch:  181  Batches:  1810  Loss:  0.00023511741892434657\n",
            "Epoch:  182  Batches:  1811  Loss:  0.00021748435392510146\n",
            "Epoch:  182  Batches:  1812  Loss:  0.00019830653036478907\n",
            "Epoch:  182  Batches:  1813  Loss:  0.00017546115850564092\n",
            "Epoch:  182  Batches:  1814  Loss:  0.00021752723841927946\n",
            "Epoch:  182  Batches:  1815  Loss:  0.00026084884302690625\n",
            "Epoch:  182  Batches:  1816  Loss:  0.0003377282992005348\n",
            "Epoch:  182  Batches:  1817  Loss:  0.00021741328237112612\n",
            "Epoch:  182  Batches:  1818  Loss:  0.0001767972280504182\n",
            "Epoch:  182  Batches:  1819  Loss:  0.0003109586541540921\n",
            "Epoch:  182  Batches:  1820  Loss:  0.0002744361700024456\n",
            "Epoch:  183  Batches:  1821  Loss:  0.00027230411069467664\n",
            "Epoch:  183  Batches:  1822  Loss:  0.00028116529574617743\n",
            "Epoch:  183  Batches:  1823  Loss:  0.00023349116963800043\n",
            "Epoch:  183  Batches:  1824  Loss:  0.0002246958902105689\n",
            "Epoch:  183  Batches:  1825  Loss:  0.000305447552818805\n",
            "Epoch:  183  Batches:  1826  Loss:  0.0003058107104152441\n",
            "Epoch:  183  Batches:  1827  Loss:  0.000299065257422626\n",
            "Epoch:  183  Batches:  1828  Loss:  0.00022413238184526563\n",
            "Epoch:  183  Batches:  1829  Loss:  0.00027605053037405014\n",
            "Epoch:  183  Batches:  1830  Loss:  0.0002994080714415759\n",
            "Epoch:  184  Batches:  1831  Loss:  0.00034002194297499955\n",
            "Epoch:  184  Batches:  1832  Loss:  0.00029535990324802697\n",
            "Epoch:  184  Batches:  1833  Loss:  0.0002981846919283271\n",
            "Epoch:  184  Batches:  1834  Loss:  0.0002667978988029063\n",
            "Epoch:  184  Batches:  1835  Loss:  0.00020220180158503354\n",
            "Epoch:  184  Batches:  1836  Loss:  0.0002032102638622746\n",
            "Epoch:  184  Batches:  1837  Loss:  0.0002540192799642682\n",
            "Epoch:  184  Batches:  1838  Loss:  0.00019713838992174715\n",
            "Epoch:  184  Batches:  1839  Loss:  0.00019744127348531038\n",
            "Epoch:  184  Batches:  1840  Loss:  0.00023079890524968505\n",
            "Epoch:  185  Batches:  1841  Loss:  0.00019745968165807426\n",
            "Epoch:  185  Batches:  1842  Loss:  0.00027952383970841765\n",
            "Epoch:  185  Batches:  1843  Loss:  0.0002849239099305123\n",
            "Epoch:  185  Batches:  1844  Loss:  0.00019361665181349963\n",
            "Epoch:  185  Batches:  1845  Loss:  0.0002910209877882153\n",
            "Epoch:  185  Batches:  1846  Loss:  0.00027413401403464377\n",
            "Epoch:  185  Batches:  1847  Loss:  0.00016726147441659123\n",
            "Epoch:  185  Batches:  1848  Loss:  0.00021321552048902959\n",
            "Epoch:  185  Batches:  1849  Loss:  0.0001972028985619545\n",
            "Epoch:  185  Batches:  1850  Loss:  0.00025168145657517016\n",
            "Epoch:  186  Batches:  1851  Loss:  0.0002225451753474772\n",
            "Epoch:  186  Batches:  1852  Loss:  0.00021838968677911907\n",
            "Epoch:  186  Batches:  1853  Loss:  0.00021059477876406163\n",
            "Epoch:  186  Batches:  1854  Loss:  0.0002721110067795962\n",
            "Epoch:  186  Batches:  1855  Loss:  0.00028255931101739407\n",
            "Epoch:  186  Batches:  1856  Loss:  0.00020066155411768705\n",
            "Epoch:  186  Batches:  1857  Loss:  0.00022108924167696387\n",
            "Epoch:  186  Batches:  1858  Loss:  0.00024309322179760784\n",
            "Epoch:  186  Batches:  1859  Loss:  0.00020819839846808463\n",
            "Epoch:  186  Batches:  1860  Loss:  0.0002784064272418618\n",
            "Epoch:  187  Batches:  1861  Loss:  0.00017413115710951388\n",
            "Epoch:  187  Batches:  1862  Loss:  0.00020349350234027952\n",
            "Epoch:  187  Batches:  1863  Loss:  0.00025005059433169663\n",
            "Epoch:  187  Batches:  1864  Loss:  0.00018520500452723354\n",
            "Epoch:  187  Batches:  1865  Loss:  0.00036032567732036114\n",
            "Epoch:  187  Batches:  1866  Loss:  0.00018518060096539557\n",
            "Epoch:  187  Batches:  1867  Loss:  0.00023157075338531286\n",
            "Epoch:  187  Batches:  1868  Loss:  0.0001685961033217609\n",
            "Epoch:  187  Batches:  1869  Loss:  0.0003882767050527036\n",
            "Epoch:  187  Batches:  1870  Loss:  0.00021401685080491006\n",
            "Epoch:  188  Batches:  1871  Loss:  0.00019089653505943716\n",
            "Epoch:  188  Batches:  1872  Loss:  0.00019879027968272567\n",
            "Epoch:  188  Batches:  1873  Loss:  0.00024836641387082636\n",
            "Epoch:  188  Batches:  1874  Loss:  0.0002741941425483674\n",
            "Epoch:  188  Batches:  1875  Loss:  0.00021942032617516816\n",
            "Epoch:  188  Batches:  1876  Loss:  0.0002465791185386479\n",
            "Epoch:  188  Batches:  1877  Loss:  0.00025583046954125166\n",
            "Epoch:  188  Batches:  1878  Loss:  0.0001818992750486359\n",
            "Epoch:  188  Batches:  1879  Loss:  0.00022393786639440805\n",
            "Epoch:  188  Batches:  1880  Loss:  0.0002464485296513885\n",
            "Epoch:  189  Batches:  1881  Loss:  0.0002028620074270293\n",
            "Epoch:  189  Batches:  1882  Loss:  0.00015293875185307115\n",
            "Epoch:  189  Batches:  1883  Loss:  0.0002654133422765881\n",
            "Epoch:  189  Batches:  1884  Loss:  0.00022157597413752228\n",
            "Epoch:  189  Batches:  1885  Loss:  0.00019719639385584742\n",
            "Epoch:  189  Batches:  1886  Loss:  0.00021715796901844442\n",
            "Epoch:  189  Batches:  1887  Loss:  0.00020956122898496687\n",
            "Epoch:  189  Batches:  1888  Loss:  0.00024061734438873827\n",
            "Epoch:  189  Batches:  1889  Loss:  0.00020034969202242792\n",
            "Epoch:  189  Batches:  1890  Loss:  0.00022643043485004455\n",
            "Epoch:  190  Batches:  1891  Loss:  0.0002593874232843518\n",
            "Epoch:  190  Batches:  1892  Loss:  0.0003068211954087019\n",
            "Epoch:  190  Batches:  1893  Loss:  0.000283703557215631\n",
            "Epoch:  190  Batches:  1894  Loss:  0.000180602801265195\n",
            "Epoch:  190  Batches:  1895  Loss:  0.00017030994058586657\n",
            "Epoch:  190  Batches:  1896  Loss:  0.00020433227473404258\n",
            "Epoch:  190  Batches:  1897  Loss:  0.0003079407033510506\n",
            "Epoch:  190  Batches:  1898  Loss:  0.0001986282877624035\n",
            "Epoch:  190  Batches:  1899  Loss:  0.00024731928715482354\n",
            "Epoch:  190  Batches:  1900  Loss:  0.00021282276429701596\n",
            "Epoch:  191  Batches:  1901  Loss:  0.0002645589702297002\n",
            "Epoch:  191  Batches:  1902  Loss:  0.0002057334640994668\n",
            "Epoch:  191  Batches:  1903  Loss:  0.00025081491912715137\n",
            "Epoch:  191  Batches:  1904  Loss:  0.0002722460776567459\n",
            "Epoch:  191  Batches:  1905  Loss:  0.00026138333487324417\n",
            "Epoch:  191  Batches:  1906  Loss:  0.0002288872201461345\n",
            "Epoch:  191  Batches:  1907  Loss:  0.00017914643103722483\n",
            "Epoch:  191  Batches:  1908  Loss:  0.0002622679457999766\n",
            "Epoch:  191  Batches:  1909  Loss:  0.00023140243138186634\n",
            "Epoch:  191  Batches:  1910  Loss:  0.00021224969532340765\n",
            "Epoch:  192  Batches:  1911  Loss:  0.0002845873241312802\n",
            "Epoch:  192  Batches:  1912  Loss:  0.00022487121168524027\n",
            "Epoch:  192  Batches:  1913  Loss:  0.00018428835028316826\n",
            "Epoch:  192  Batches:  1914  Loss:  0.0002056722150882706\n",
            "Epoch:  192  Batches:  1915  Loss:  0.00023643075837753713\n",
            "Epoch:  192  Batches:  1916  Loss:  0.00027484347810968757\n",
            "Epoch:  192  Batches:  1917  Loss:  0.00020400069479364902\n",
            "Epoch:  192  Batches:  1918  Loss:  0.0001811675465432927\n",
            "Epoch:  192  Batches:  1919  Loss:  0.00023911381140351295\n",
            "Epoch:  192  Batches:  1920  Loss:  0.00021820262190885842\n",
            "Epoch:  193  Batches:  1921  Loss:  0.0003000106371473521\n",
            "Epoch:  193  Batches:  1922  Loss:  0.00013012222188990563\n",
            "Epoch:  193  Batches:  1923  Loss:  0.00017075028154067695\n",
            "Epoch:  193  Batches:  1924  Loss:  0.00018480200378689915\n",
            "Epoch:  193  Batches:  1925  Loss:  0.00020941102411597967\n",
            "Epoch:  193  Batches:  1926  Loss:  0.0002843078691512346\n",
            "Epoch:  193  Batches:  1927  Loss:  0.00021970125089865178\n",
            "Epoch:  193  Batches:  1928  Loss:  0.00021528992510866374\n",
            "Epoch:  193  Batches:  1929  Loss:  0.00026377392350696027\n",
            "Epoch:  193  Batches:  1930  Loss:  0.00020799612684641033\n",
            "Epoch:  194  Batches:  1931  Loss:  0.00021707186533603817\n",
            "Epoch:  194  Batches:  1932  Loss:  0.000227929835091345\n",
            "Epoch:  194  Batches:  1933  Loss:  0.00016713447985239327\n",
            "Epoch:  194  Batches:  1934  Loss:  0.00019072323630098253\n",
            "Epoch:  194  Batches:  1935  Loss:  0.00027212200802750885\n",
            "Epoch:  194  Batches:  1936  Loss:  0.00022348470520228148\n",
            "Epoch:  194  Batches:  1937  Loss:  0.00020746351219713688\n",
            "Epoch:  194  Batches:  1938  Loss:  0.00022789943614043295\n",
            "Epoch:  194  Batches:  1939  Loss:  0.0002020784158958122\n",
            "Epoch:  194  Batches:  1940  Loss:  0.00017762491188477725\n",
            "Epoch:  195  Batches:  1941  Loss:  0.0002081175916828215\n",
            "Epoch:  195  Batches:  1942  Loss:  0.00023427624546457082\n",
            "Epoch:  195  Batches:  1943  Loss:  0.0002302749053342268\n",
            "Epoch:  195  Batches:  1944  Loss:  0.00014859068323858082\n",
            "Epoch:  195  Batches:  1945  Loss:  0.00019156106282025576\n",
            "Epoch:  195  Batches:  1946  Loss:  0.00019620545208454132\n",
            "Epoch:  195  Batches:  1947  Loss:  0.00016640468675177544\n",
            "Epoch:  195  Batches:  1948  Loss:  0.00022747089678887278\n",
            "Epoch:  195  Batches:  1949  Loss:  0.00023418792989104986\n",
            "Epoch:  195  Batches:  1950  Loss:  0.00024243892403319478\n",
            "Epoch:  196  Batches:  1951  Loss:  0.00017558886611368507\n",
            "Epoch:  196  Batches:  1952  Loss:  0.00028940913034603\n",
            "Epoch:  196  Batches:  1953  Loss:  0.00022494973381981254\n",
            "Epoch:  196  Batches:  1954  Loss:  0.0002888162562157959\n",
            "Epoch:  196  Batches:  1955  Loss:  0.0002150470099877566\n",
            "Epoch:  196  Batches:  1956  Loss:  0.0002419030643068254\n",
            "Epoch:  196  Batches:  1957  Loss:  0.000207496719667688\n",
            "Epoch:  196  Batches:  1958  Loss:  0.0002046846057055518\n",
            "Epoch:  196  Batches:  1959  Loss:  0.00019752309890463948\n",
            "Epoch:  196  Batches:  1960  Loss:  0.0001628415921004489\n",
            "Epoch:  197  Batches:  1961  Loss:  0.00024603179190307856\n",
            "Epoch:  197  Batches:  1962  Loss:  0.0001732778619043529\n",
            "Epoch:  197  Batches:  1963  Loss:  0.00016493424482177943\n",
            "Epoch:  197  Batches:  1964  Loss:  0.00022684698342345655\n",
            "Epoch:  197  Batches:  1965  Loss:  0.0001608330785529688\n",
            "Epoch:  197  Batches:  1966  Loss:  0.00020833153394050896\n",
            "Epoch:  197  Batches:  1967  Loss:  0.0002607628412079066\n",
            "Epoch:  197  Batches:  1968  Loss:  0.00018613028805702925\n",
            "Epoch:  197  Batches:  1969  Loss:  0.00021262170048430562\n",
            "Epoch:  197  Batches:  1970  Loss:  0.00021894719975534827\n",
            "Epoch:  198  Batches:  1971  Loss:  0.00026479680673219264\n",
            "Epoch:  198  Batches:  1972  Loss:  0.0002528507320675999\n",
            "Epoch:  198  Batches:  1973  Loss:  0.0002055787481367588\n",
            "Epoch:  198  Batches:  1974  Loss:  0.00016361822781618685\n",
            "Epoch:  198  Batches:  1975  Loss:  0.00025864646886475384\n",
            "Epoch:  198  Batches:  1976  Loss:  0.0002441637043375522\n",
            "Epoch:  198  Batches:  1977  Loss:  0.00019979830540250987\n",
            "Epoch:  198  Batches:  1978  Loss:  0.00018094078404828906\n",
            "Epoch:  198  Batches:  1979  Loss:  0.0002041692059719935\n",
            "Epoch:  198  Batches:  1980  Loss:  0.0001837193121900782\n",
            "Epoch:  199  Batches:  1981  Loss:  0.00017972203204408288\n",
            "Epoch:  199  Batches:  1982  Loss:  0.00018013623775914311\n",
            "Epoch:  199  Batches:  1983  Loss:  0.00024086178746074438\n",
            "Epoch:  199  Batches:  1984  Loss:  0.00022614178305957466\n",
            "Epoch:  199  Batches:  1985  Loss:  0.00018967453797813505\n",
            "Epoch:  199  Batches:  1986  Loss:  0.00018143211491405964\n",
            "Epoch:  199  Batches:  1987  Loss:  0.00018299162911716849\n",
            "Epoch:  199  Batches:  1988  Loss:  0.00020075197971891612\n",
            "Epoch:  199  Batches:  1989  Loss:  0.00021975088748149574\n",
            "Epoch:  199  Batches:  1990  Loss:  0.00028817675774917006\n",
            "Epoch:  200  Batches:  1991  Loss:  0.00021193365682847798\n",
            "Epoch:  200  Batches:  1992  Loss:  0.0002141554286936298\n",
            "Epoch:  200  Batches:  1993  Loss:  0.00023171845532488078\n",
            "Epoch:  200  Batches:  1994  Loss:  0.0002321726642549038\n",
            "Epoch:  200  Batches:  1995  Loss:  0.00020386563846841455\n",
            "Epoch:  200  Batches:  1996  Loss:  0.0002501610724721104\n",
            "Epoch:  200  Batches:  1997  Loss:  0.00018982068286277354\n",
            "Epoch:  200  Batches:  1998  Loss:  0.00018152903066948056\n",
            "Epoch:  200  Batches:  1999  Loss:  0.00021535133419092745\n",
            "Epoch:  200  Batches:  2000  Loss:  0.00020267012587282807\n",
            "The loss of training is:  0.00020267012587282807\n",
            "The loss of validation is:  0.00022700648696627468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z4ZjzbrD-ao",
        "colab_type": "text"
      },
      "source": [
        "**Perform Predictions of a given input (numpy)**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCJpWviy-NxF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "f34283bd-a454-4d98-eb79-3671b859d62e"
      },
      "source": [
        "\n",
        "#This is how you make predicitons\n",
        "fs = ANN.predict(X)\n",
        "\n",
        "plt.plot(F,fs,'o')\n",
        "plt.xlabel('Labels')\n",
        "plt.ylabel('Predicitons')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Predicitons')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTWOiI3q-7cj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}